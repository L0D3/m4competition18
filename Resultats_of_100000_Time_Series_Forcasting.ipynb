{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **********Model  0 : LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True) ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Date    ######################\n",
      " Time Serie nr. 0  sMape:  0.07171083690749604      MAsE:  1.1002672844144492\n",
      " Time Serie nr. 1  sMape:  0.04330791183577994      MAsE:  0.730753897310295\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.057509374371637986\n",
      "MAsE:   0.9155105908623722\n",
      "###################      Daily_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.006490015298321406      MAsE:  2.9705969719427716\n",
      " Time Serie nr. 1  sMape:  0.010767491197288226      MAsE:  2.2080998466144655\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.008628753247804816\n",
      "MAsE:   2.5893484092786183\n",
      "###################      Weekly_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.07843711877636238      MAsE:  46.39543846972958\n",
      " Time Serie nr. 1  sMape:  0.020627607722499084      MAsE:  6.543687694137004\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.04953236324943073\n",
      "MAsE:   26.46956308193329\n",
      "###################      Monthly_Daten     ######################\n",
      " Time Serie nr. 0  sMape:  0.10670657603289768      MAsE:  0.9292961138160072\n",
      " Time Serie nr. 1  sMape:  0.1843616656720158      MAsE:  1.151221485100066\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.14553412085245676\n",
      "MAsE:   1.0402587994580366\n",
      "###################      Quaterly_Daten    ######################\n",
      " Time Serie nr. 0  sMape:  0.4431020576204102      MAsE:  7.093195386816962\n",
      " Time Serie nr. 1  sMape:  0.06688304555392663      MAsE:  1.1797813340519667\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.25499255158716844\n",
      "MAsE:   4.136488360434464\n",
      "###################      Yearly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.01913302270753421      MAsE:  1.268114244213451\n",
      " Time Serie nr. 1  sMape:  0.2711236149458743      MAsE:  1.6512100989040155\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.14512831882670424\n",
      "MAsE:   1.4596621715587332\n",
      "\n",
      " **********Model  1 : DecisionTreeRegressor(criterion='mae', max_depth=28, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=5,\n",
      "           min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Date    ######################\n",
      " Time Serie nr. 0  sMape:  0.06593473663803834      MAsE:  1.0540571539902208\n",
      " Time Serie nr. 1  sMape:  0.0400442490627253      MAsE:  0.7182643119107207\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.05298949285038182\n",
      "MAsE:   0.8861607329504708\n",
      "###################      Daily_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.006039112627858056      MAsE:  2.7547931728745167\n",
      " Time Serie nr. 1  sMape:  0.010163596823241609      MAsE:  2.099526490079299\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.008101354725549832\n",
      "MAsE:   2.427159831476908\n",
      "###################      Weekly_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.02350135922372573      MAsE:  14.184486376291465\n",
      " Time Serie nr. 1  sMape:  0.04461378672675932      MAsE:  14.558534946293127\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.03405757297524253\n",
      "MAsE:   14.371510661292296\n",
      "###################      Monthly_Daten     ######################\n",
      " Time Serie nr. 0  sMape:  0.12277766815842026      MAsE:  1.089560301900974\n",
      " Time Serie nr. 1  sMape:  0.20106798938301815      MAsE:  1.2649478683883393\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.1619228287707192\n",
      "MAsE:   1.1772540851446567\n",
      "###################      Quaterly_Daten    ######################\n",
      " Time Serie nr. 0  sMape:  0.07937348879071957      MAsE:  1.7937391087981518\n",
      " Time Serie nr. 1  sMape:  0.07886430930919754      MAsE:  1.4495428037774136\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.07911889904995856\n",
      "MAsE:   1.6216409562877827\n",
      "###################      Yearly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.03608871985767804      MAsE:  2.3630528606735735\n",
      " Time Serie nr. 1  sMape:  0.32633740742853606      MAsE:  2.026315789473684\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.18121306364310705\n",
      "MAsE:   2.194684325073629\n",
      "\n",
      " **********Model  2 : KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "          weights='uniform') ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Date    ######################\n",
      " Time Serie nr. 0  sMape:  0.05217345615048871      MAsE:  0.845021808263258\n",
      " Time Serie nr. 1  sMape:  0.049554063352753724      MAsE:  0.8809209677165475\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.05086375975162122\n",
      "MAsE:   0.8629713879899028\n",
      "###################      Daily_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.00663847449033204      MAsE:  3.0268715109362088\n",
      " Time Serie nr. 1  sMape:  0.016825278627428934      MAsE:  3.491704932090403\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.011731876558880487\n",
      "MAsE:   3.259288221513306\n",
      "###################      Weekly_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.015918701044354177      MAsE:  9.587992482581274\n",
      " Time Serie nr. 1  sMape:  0.018394421900657466      MAsE:  5.817497133921844\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.01715656147250582\n",
      "MAsE:   7.702744808251559\n",
      "###################      Monthly_Daten     ######################\n",
      " Time Serie nr. 0  sMape:  0.10101208435664082      MAsE:  0.9025057502461906\n",
      " Time Serie nr. 1  sMape:  0.1456752443778443      MAsE:  0.8946115099125511\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.12334366436724256\n",
      "MAsE:   0.8985586300793709\n",
      "###################      Quaterly_Daten    ######################\n",
      " Time Serie nr. 0  sMape:  0.08884016265551892      MAsE:  2.019131570067564\n",
      " Time Serie nr. 1  sMape:  0.09354668765831026      MAsE:  1.7346429336516482\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.09119342515691459\n",
      "MAsE:   1.876887251859606\n",
      "###################      Yearly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.02896347810409393      MAsE:  1.9049909624110002\n",
      " Time Serie nr. 1  sMape:  0.30261825706424994      MAsE:  1.868251273344652\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.16579086758417194\n",
      "MAsE:   1.886621117877826\n",
      "\n",
      " **********Model  3 : BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=True, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False) ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Date    ######################\n",
      " Time Serie nr. 0  sMape:  0.07069958695502289      MAsE:  1.0886077685597986\n",
      " Time Serie nr. 1  sMape:  0.04339543333528276      MAsE:  0.7313104452794038\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.05704751014515282\n",
      "MAsE:   0.9099591069196011\n",
      "###################      Daily_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.006381078057332865      MAsE:  2.9221144140449513\n",
      " Time Serie nr. 1  sMape:  0.011034594608274701      MAsE:  2.2613720073193835\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.008707836332803783\n",
      "MAsE:   2.5917432106821674\n",
      "###################      Weekly_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.14048337406455585      MAsE:  77.20826514226222\n",
      " Time Serie nr. 1  sMape:  0.021239507242938833      MAsE:  6.7318394845505\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.08086144065374734\n",
      "MAsE:   41.97005231340636\n",
      "###################      Monthly_Daten     ######################\n",
      " Time Serie nr. 0  sMape:  0.09588122751739413      MAsE:  0.851315264986656\n",
      " Time Serie nr. 1  sMape:  0.17775634830651357      MAsE:  1.097665651620856\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.13681878791195384\n",
      "MAsE:   0.974490458303756\n",
      "###################      Quaterly_Daten    ######################\n",
      " Time Serie nr. 0  sMape:  0.053079812513890365      MAsE:  1.1147191026654795\n",
      " Time Serie nr. 1  sMape:  0.01818492549498387      MAsE:  0.3237476512727843\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.035632369004437114\n",
      "MAsE:   0.7192333769691319\n",
      "###################      Yearly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.019093081581182628      MAsE:  1.2652961544403867\n",
      " Time Serie nr. 1  sMape:  0.26244145219653375      MAsE:  1.5860913285438274\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.14076726688885818\n",
      "MAsE:   1.4256937414921071\n",
      "\n",
      " **********Model  4 : SVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.06532323390138434      MAsE:  1.056473502038256\n",
      " Time Serie nr. 1  sMape:  0.05748792832844282      MAsE:  1.1119333533859448\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.06140558111491358\n",
      "MAsE:   1.0842034277121004\n",
      "###################      Daily_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.06393150402298707      MAsE:  28.303791119832507\n",
      " Time Serie nr. 1  sMape:  0.008340740176203672      MAsE:  1.7186570522147107\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.03613612209959537\n",
      "MAsE:   15.011224086023608\n",
      "###################      Weekly_Daten      ######################\n",
      " Time Serie nr. 0  sMape:  0.12316197819102324      MAsE:  70.60349323495016\n",
      " Time Serie nr. 1  sMape:  0.09817246674425208      MAsE:  30.063103528522074\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.11066722246763766\n",
      "MAsE:   50.33329838173612\n",
      "###################      Monthly_Daten       ######################\n",
      " Time Serie nr. 0  sMape:  0.17850917362178376      MAsE:  1.6301894982110434\n",
      " Time Serie nr. 1  sMape:  0.1438425715361532      MAsE:  0.879601018218165\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.16117587257896848\n",
      "MAsE:   1.2548952582146042\n",
      "###################      Quaterly_Daten     ######################\n",
      " Time Serie nr. 0  sMape:  1.209546250527877      MAsE:  15.575839172305011\n",
      " Time Serie nr. 1  sMape:  0.20674892579046406      MAsE:  3.8149784613886215\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.7081475881591704\n",
      "MAsE:   9.695408816846816\n",
      "###################      Yearly_Daten        ######################\n",
      " Time Serie nr. 0  sMape:  0.03428605511140107      MAsE:  2.2474749367232336\n",
      " Time Serie nr. 1  sMape:  0.3955510560500637      MAsE:  2.512008329576618\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.2149185555807324\n",
      "MAsE:   2.3797416331499255\n",
      "\n",
      " **********Model  5 : MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=6, learning_rate='adaptive',\n",
      "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False) ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Daten        ######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:137: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:138: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time Serie nr. 0  sMape:  0.04318161436049372      MAsE:  0.7509807572401253\n",
      " Time Serie nr. 1  sMape:  0.06688828535505102      MAsE:  1.187601269863938\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.05503494985777237\n",
      "MAsE:   0.9692910135520316\n",
      "\n",
      "\n",
      "###################      Daily_Daten        ######################\n",
      " Time Serie nr. 0  sMape:  0.009251672952092883      MAsE:  4.243127093809655\n",
      " Time Serie nr. 1  sMape:  0.010831876622931199      MAsE:  2.236112155826693\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.010041774787512042\n",
      "MAsE:   3.239619624818174\n",
      "\n",
      "\n",
      "###################      Weekly_Daten        ######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:141: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time Serie nr. 0  sMape:  0.025078327698874987      MAsE:  15.268650810594066\n",
      " Time Serie nr. 1  sMape:  0.030899380226532415      MAsE:  9.961448896294558\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.027988853962703703\n",
      "MAsE:   12.615049853444312\n",
      "\n",
      "\n",
      "###################      Monthly_Daten        ######################\n",
      " Time Serie nr. 0  sMape:  0.17966518728594807      MAsE:  1.713323710616901\n",
      " Time Serie nr. 1  sMape:  0.21895539393471453      MAsE:  1.4096123912319667\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.1993102906103313\n",
      "MAsE:   1.5614680509244339\n",
      "\n",
      "\n",
      "###################      Quaterly_Daten        ######################\n",
      " Time Serie nr. 0  sMape:  0.07413383609160289      MAsE:  1.5097667966578703\n",
      " Time Serie nr. 1  sMape:  0.0751816989748067      MAsE:  1.2547154174123325\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.0746577675332048\n",
      "MAsE:   1.3822411070351013\n",
      "\n",
      "\n",
      "###################      Yearly_Daten        ######################\n",
      " Time Serie nr. 0  sMape:  0.12165497853256883      MAsE:  8.610370750523046\n",
      " Time Serie nr. 1  sMape:  0.34764920784502323      MAsE:  1.884863358043944\n",
      "\n",
      "  Anzahl der Serie:  2\n",
      "sMAPE:   0.23465209318879604\n",
      "MAsE:   5.247617054283495\n",
      "\n",
      "\n",
      "\n",
      " **********Model  6 : SimpleRNN ***************\n",
      "\n",
      "\n",
      "###################      Hourly_Daten        ######################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:137: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=24,center=True).mean()\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:138: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=2,center=True).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 857.8681\n",
      "Epoch 2/100\n",
      "512/512 [==============================] - 14s 26ms/step - loss: 245.0547\n",
      "Epoch 3/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 158.0443\n",
      "Epoch 4/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 120.8061\n",
      "Epoch 5/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 102.0258\n",
      "Epoch 6/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 94.8474\n",
      "Epoch 7/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 91.1394\n",
      "Epoch 8/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 91.1677\n",
      "Epoch 9/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 91.4561\n",
      "Epoch 10/100\n",
      "512/512 [==============================] - 13s 25ms/step - loss: 90.5581\n",
      "Epoch 11/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 88.7581\n",
      "Epoch 12/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 88.5870\n",
      "Epoch 13/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 89.3580\n",
      "Epoch 14/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 90.4024\n",
      "Epoch 15/100\n",
      "512/512 [==============================] - 13s 26ms/step - loss: 88.7419\n",
      "Epoch 16/100\n",
      "512/512 [==============================] - 14s 27ms/step - loss: 89.2416\n",
      "Epoch 17/100\n",
      "512/512 [==============================] - 14s 26ms/step - loss: 88.9900\n",
      "Epoch 18/100\n",
      " 25/512 [>.............................] - ETA: 13s - loss: 70.8673"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3941ff9aaacc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m \u001b[0mglobal_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3941ff9aaacc>\u001b[0m in \u001b[0;36mglobal_prediction\u001b[0;34m()\u001b[0m\n\u001b[1;32m    843\u001b[0m      \u001b[0;31m# Hourly Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###################      Hourly_Daten        ######################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0msMape_hourly_general\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMase_hourly_general\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_predict_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_hourly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0mT_sMape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msMape_hourly_general\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0mT_Mase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMase_hourly_general\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3941ff9aaacc>\u001b[0m in \u001b[0;36mmain_predict_rnn\u001b[0;34m(Data, fh, freq)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Prediction with Linear regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_bench\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m      \u001b[0;31m# ==== add trend ====#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3941ff9aaacc>\u001b[0m in \u001b[0;36mrnn_bench\u001b[0;34m(x_train, y_train, x_test, fh, input_size)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# fit the model to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Laden von der verschieden Bibliothek zur Daten Visualizierung und Vorhersagen\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression ,BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.optimizers import rmsprop\n",
    "from keras import backend as ker\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "os.chdir('/home/jupyter/jupyterNotebooks/m4competition18/data')\n",
    "#os.getcwd()\n",
    "\n",
    "# Load of Hourly Data\n",
    "df_hourly = pd.read_csv(\"Hourly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_hourly = df_hourly.T\n",
    "\n",
    "## Laden von Dataset Weekly-train\n",
    "df_weekly = pd.read_csv(\"Weekly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_weekly = df_weekly.T\n",
    "\n",
    "## Laden von  Dataset Yearly-train\n",
    "df_yearly = pd.read_csv(\"Yearly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_yearly = df_yearly.T\n",
    "\n",
    "# Load of monthly Data\n",
    "df_monthly = pd.read_csv(\"Monthly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_monthly = df_monthly.T\n",
    "\n",
    "## Laden von Dataset Quarterly-train\n",
    "df_quaterly = pd.read_csv(\"Quarterly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_quaterly = df_quaterly.T\n",
    "\n",
    "# load of Daily Data\n",
    "df_daily = pd.read_csv(\"Daily-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_daily = df_daily.T\n",
    "\n",
    "def remov_nan (dataset):\n",
    "    '''\n",
    "    to remove all NaN Values in a \n",
    "    Time Serie Dataframe\n",
    "    '''\n",
    "    n = dataset.isnull().sum() \n",
    "    data = dataset[0:(len(dataset)-n)]\n",
    "    return data\n",
    "\n",
    "def copy_val(x):\n",
    "    '''\n",
    "    to copy a list or array in a new memory \n",
    "    without reference \n",
    "    x: list or array\n",
    "    '''\n",
    "    y =[]\n",
    "    for i in x:\n",
    "        y.append(i)\n",
    "    return np.array(y)\n",
    "\n",
    "def normalisieren_data(dataset):\n",
    "    '''\n",
    "    to normalize Data\n",
    "    : dataset : Data to normalize\n",
    "    ''' \n",
    "    scaler = scaler =MinMaxScaler(feature_range=(0, 1)).fit(dataset)\n",
    "    Dataset_normalized = scaler.transform(dataset)\n",
    "    return Dataset_normalized,scaler\n",
    "\n",
    "def detrend(insample_data):\n",
    "    \"\"\"\n",
    "    Calculates a & b parameters of LRL\n",
    "    :param insample_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = np.arange(len(insample_data))\n",
    "    a, b = np.polyfit(x, insample_data, 1)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def deseasonalize(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Calculates and returns seasonal indices\n",
    "    :param original_ts: original data\n",
    "    :param ppy: periods per year\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # === get in-sample data\n",
    "    original_ts = original_ts[:-out_of_sample]\n",
    "    \"\"\"\n",
    "    if seasonality_test(original_ts, ppy):\n",
    "        # print(\"seasonal\")\n",
    "        # ==== get moving averages\n",
    "        ma_ts = moving_averages(original_ts, ppy)\n",
    "\n",
    "        # ==== get seasonality indices\n",
    "        le_ts = original_ts * 100 / ma_ts\n",
    "        le_ts = np.hstack((le_ts, np.full((ppy - (len(le_ts) % ppy)), np.nan)))\n",
    "        le_ts = np.reshape(le_ts, (-1, ppy))\n",
    "        si = np.nanmean(le_ts, 0)\n",
    "        norm = np.sum(si) / (ppy * 100)\n",
    "        si = si / norm\n",
    "    else:\n",
    "        # print(\"NOT seasonal\")\n",
    "        si = np.full(ppy, 100)\n",
    "\n",
    "    return si\n",
    "\n",
    "\n",
    "## BENCHMARK ##\n",
    "def moving_averages(ts_init, window):\n",
    "    \"\"\"\n",
    "    Calculates the moving averages for a given TS\n",
    "    :param ts_init: the original time series\n",
    "    :param window: window length\n",
    "    :return: moving averages ts\n",
    "    \"\"\"\n",
    "    if len(ts_init) % 2 == 0:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "        ts_ma = pd.rolling_mean(ts_ma, 2, center=True)\n",
    "        ts_ma = np.roll(ts_ma, -1)\n",
    "    else:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "\n",
    "    return ts_ma\n",
    "\n",
    "def seasonality_test(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Seasonality test\n",
    "    :param original_ts: time series\n",
    "    :param ppy: periods per year\n",
    "    :return: boolean value: whether the TS is seasonal\n",
    "    \"\"\"\n",
    "    s = acf(original_ts, 1)\n",
    "    for i in range(2, ppy):\n",
    "        s = s + (acf(original_ts, i) ** 2)\n",
    "\n",
    "    limit = 1.645 * (sqrt((1 + 2 * s) / len(original_ts)))\n",
    "\n",
    "    return (abs(acf(original_ts, ppy))) > limit\n",
    "\n",
    "\n",
    "def acf(data, k):\n",
    "    \"\"\"\n",
    "    Autocorrelation function\n",
    "    :param data: time series\n",
    "    :param k: lag\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = np.mean(data)\n",
    "    s1 = 0\n",
    "    for i in range(k, len(data)):\n",
    "        s1 = s1 + ((data[i] - m) * (data[i - k] - m))\n",
    "\n",
    "    s2 = 0\n",
    "    for i in range(0, len(data)):\n",
    "        s2 = s2 + ((data[i] - m) ** 2)\n",
    "\n",
    "    return float(s1 / s2)\n",
    "\n",
    "## BENCHMARK ##\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item() \n",
    "\n",
    "##===Mean Absolute Scaled Error ====##\n",
    "def mase(insample, y_test, y_hat_test, freq):\n",
    "    \"\"\"\n",
    "    Calculates MAsE\n",
    "    :param insample: insample data\n",
    "    :param y_test: out of sample target values\n",
    "    :param y_hat_test: predicted values\n",
    "    :param freq: data frequency\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_naive = []\n",
    "    for i in range(freq, len(insample)):\n",
    "        y_hat_naive.append(insample[(i - freq)])\n",
    "    masep = np.mean(abs(insample[freq:] - y_hat_naive))\n",
    "    return np.mean(abs(y_test - y_hat_test)) / masep\n",
    "#Hier wird die \"Time serie\" als \"spervised learning Problem\" umgewandel.\n",
    "#Die Datenmenge der Zeitreihen wird in Training und Testing Datamenge und jeweils in input & output Daten\n",
    "\n",
    "# Hilfsfunktion , die eine Datenmenge in input und output Menge aufteile \n",
    "def split_input_output(dataset: np.ndarray, in_back: int=1) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" \n",
    "    The function takes two arguments: the `dataset`, which is a NumPy array that we want to convert into a dataset,\n",
    "    and the `in_back`, which is the number of previous time steps to use as input variables\n",
    "    to predict the next time period — in this case defaulted to 1.\n",
    "    :dataset: numpy dataset\n",
    "    :in_variable: number of previous time steps as int\n",
    "    :return: tuple of input and output dataset\n",
    "    \"\"\"\n",
    "    Input, Output = [], []\n",
    "    for i in range(len(dataset)-in_back):\n",
    "        a = dataset[i:(i+in_back)]\n",
    "        Input.append(a)\n",
    "        Output.append(dataset[i + in_back])\n",
    "    return np.array(Input), np.array(Output)\n",
    "\n",
    "## Folgende Funktion split die Datenmende in Training and Testing Daten.\n",
    "\n",
    "def split_into_train_test(dataset: np.ndarray,train_size, in_back) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test datasets. \n",
    "    : dataset: (np.ndarray) Time serie Dataset \n",
    "    : train_size: (int) Größe der Training Datamenge\n",
    "    : look_back: (int) number of previous time steps \n",
    "    :return: tuple of training data and test dataset\n",
    "    \"\"\"\n",
    "    if not train_size > in_back:\n",
    "        raise ValueError('train_size muss größer als look_back',\"train_size:\",train_size,\"in_back:\",in_back)\n",
    "    train= dataset[0:train_size]\n",
    "    test = dataset[train_size - in_back:len(dataset)]\n",
    "    #print('train_dataset: {}, test_dataset: {}'.format(len(train), len(test)))\n",
    "    return train, test\n",
    "\n",
    "## Es wird hier die Datenmenge in X_train,Y_train für das Training und X_test,Y_test für das Testing \n",
    "\n",
    "def all_split (dataset: np.ndarray,fh, in_back) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits dataset into input-training (X_train), outout_training(Y_train) and input_test(X_test) , output_test(Y_test) datasets.\n",
    "    : dataset:(np.ndarray) Time serie Dataset\n",
    "    :df:(float64) Größe der Testing Datamenge \n",
    "    : in_back: (int) number of previous time steps \n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    #if not (size_prozent>0 and size_prozent<1):\n",
    "        #raise ValueError('size_prozent of training must be in the interval 0 and 1')\n",
    "    train_size = len(dataset)-fh\n",
    "    training, testing = split_into_train_test(dataset,train_size,in_back)\n",
    "    X_train, Y_train = split_input_output(training,in_back)\n",
    "    X_test, Y_test = split_input_output(testing,in_back)\n",
    "    return X_train,Y_train,X_test[0].reshape(1,-1),Y_test\n",
    "\n",
    "def check_pred (dataset: pd.DataFrame,y_pred: np.ndarray):\n",
    "    ''''\n",
    "    this function check the negativity of the predicted values, set them to null \n",
    "    if they are negativ and to max value of the serie data if they are extrem high\n",
    "    : dataset: Dataset of the serie\n",
    "    : y_pred:  The list of predicted values\n",
    "    : return:\n",
    "    '''\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0:\n",
    "            y_pred[i]=0\n",
    "        if y_pred[i]> (9000*max(dataset)):\n",
    "            y_pred[i]=max(dataset)\n",
    "\n",
    "def Lin_Reg():\n",
    "    model = LinearRegression(normalize=True)\n",
    "    return model\n",
    "\n",
    "def Ridge_Regression():\n",
    "    model =  BayesianRidge(compute_score=True)\n",
    "    return model   \n",
    "\n",
    "def Dtree_Regression():\n",
    "    model = DecisionTreeRegressor(criterion='mae',max_depth=28,min_samples_split=5,\n",
    "                                 min_samples_leaf =5)\n",
    "    return model\n",
    "\n",
    "\n",
    "def K_neaRegression ():\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    return model\n",
    "\n",
    "\n",
    "def SVM_Regression ():\n",
    "    model =  SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    return model \n",
    "\n",
    "## BENCHMARK ##\n",
    "\n",
    "def mlp_benchm():\n",
    "    model = MLPRegressor(hidden_layer_sizes=6, activation='identity', solver='adam',\n",
    "                         max_iter=100, learning_rate='adaptive', learning_rate_init=0.001,\n",
    "                         random_state=42)\n",
    "    return model\n",
    "\n",
    "def rnn_benchm(input_size: int=3):\n",
    "     model = Sequential([\n",
    "        SimpleRNN(6, input_shape=(input_size, 1), activation='linear',\n",
    "                  use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                  recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  dropout=0.0, recurrent_dropout=0.0),\n",
    "        Dense(1, use_bias=True, activation='linear')\n",
    "    ])\n",
    "    opt = rmsprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def rnn_bench(x_train, y_train, x_test, fh, input_size):\n",
    "    \"\"\"\n",
    "    Forecasts using 6 SimpleRNN nodes in the hidden layer and a Dense output layer\n",
    "    :param x_train: train data\n",
    "    :param y_train: target values for training\n",
    "    :param x_test: test data\n",
    "    :param fh: forecasting horizon\n",
    "    :param input_size: number of points used as input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reshape to match expected input\n",
    "    x_train = np.reshape(x_train, (-1, input_size, 1))\n",
    "    x_test = np.reshape(x_test, (-1, input_size, 1))\n",
    "    # create the model\n",
    "    model = Sequential([\n",
    "        SimpleRNN(6, input_shape=(input_size, 1), activation='linear',\n",
    "                  use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                  recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  dropout=0.0, recurrent_dropout=0.0),\n",
    "        Dense(1, use_bias=True, activation='linear')\n",
    "    ])\n",
    "    opt = rmsprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    # fit the model to the training data\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1)\n",
    "    # make predictions\n",
    "    y_hat_test = []\n",
    "    last_prediction = model.predict(x_test)[0][0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0][(len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0][0]\n",
    "        #print(last_prediction)\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "def LSTM_NN(x_train, y_train, x_test, fh, in_back):\n",
    "    \n",
    "    # reshape to match expected input\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], in_back, 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], in_back, 1))\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(in_back,1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=1)\n",
    "    # make predictions\n",
    "    y_hat_test = []\n",
    "    last_prediction = model.predict(x_test)[0][0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0][(len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0][0]\n",
    "        #print(last_prediction)\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "## Eine Funktion, die die Vorhersage von mehreren Time-series mit simple Reccurent neuronal netzwerk(Benchmark model) evaluiert. \n",
    "## Sie gibt die durchschnittliche Werte der Metriken sMAPE und MASE für die Menge der Time-Series zurück.\n",
    "def main_predict_rnn (Data: pd.DataFrame,fh,freq):\n",
    "   \n",
    "    \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset\n",
    "    \n",
    "    model_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    model_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    c = 0\n",
    "    \n",
    "    # Iteration through each serie in the dataset\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        Dat = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        Data_val  = remov_nan (Dat)\n",
    "       \n",
    "        in_back = int(0.2*len(Data_val ))\n",
    "        # load the value of new_Data\n",
    "        # = new_Data.values\n",
    "        \n",
    "        # ==== remove seasonality ====#\n",
    "        seasonality_in = deseasonalize(Data_val, freq)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "        # ==== detrending ====#\n",
    "        a, b = detrend(Data_val)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] - ((a * i) + b)\n",
    "        \n",
    "        \n",
    "        # Split the data into x_train, y_train, x_test, y_test\n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        \n",
    "        # Prediction with Linear regression\n",
    "        Y_pred = rnn_bench(x_train, y_train, copy_val(x_test),fh, in_back)\n",
    "      \n",
    "     # ==== add trend ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] + ((a * i) + b)\n",
    "    \n",
    "        for i in range(0, fh):\n",
    "            Y_pred[i] = Y_pred[i] + ((a * (len(Data_val) + i + 1)) + b)\n",
    "           \n",
    "    # ==== add seasonality ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * seasonality_in[i % freq] / 100\n",
    "    \n",
    "        for i in range(len(Data_val), len(Data_val) + fh):\n",
    "            Y_pred[i - len(Data_val)] = Y_pred[i - len(Data_val)] * seasonality_in[i % freq] / 100\n",
    "        \n",
    "        #print(\"y_pre\",Y_pred)\n",
    "        #print(Data_val.head)\n",
    "    \n",
    "        # check the prediction on negativity and extremity\n",
    "        check_pred(Data_val,Y_pred)\n",
    "        \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        print(\" Time Serie nr.\",i, \" sMape: \",smape(y_test, Y_pred), \"     MAsE: \",mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        # calculation of Error\n",
    "        model_sMAPE.append(smape(y_test, Y_pred))\n",
    "        model_MASE.append(mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        c+=1\n",
    "    print(\"\\n\",\" Anzahl der Serie: \",c)\n",
    "    print(\"sMAPE:  \",np.mean(model_sMAPE))\n",
    "    print(\"MAsE:  \",np.mean(model_MASE))\n",
    "   \n",
    "    return np.mean(model_sMAPE), np.mean(model_MASE)\n",
    "\n",
    "def main_predict_lstm (Data: pd.DataFrame,fh,freq):\n",
    " \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset\n",
    "    \n",
    "    model_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    model_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    \n",
    "    c=0\n",
    "    # Iteration through each serie in the dataset\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        Dat = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        Data_val  = remov_nan (Dat)\n",
    "       \n",
    "        in_back = int(0.2*len(Data_val ))\n",
    "        # load the value of new_Data\n",
    "        # = new_Data.values\n",
    "        \n",
    "        # ==== remove seasonality ====#\n",
    "        seasonality_in = deseasonalize(Data_val, freq)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "        # ==== detrending ====#\n",
    "        a, b = detrend(Data_val)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] - ((a * i) + b)\n",
    "        \n",
    "        \n",
    "        # Split the data into x_train, y_train, x_test, y_test\n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        \n",
    "        # Prediction with Linear regression\n",
    "        Y_pred = LSTM_NN(x_train, y_train, copy_val(x_test),fh, in_back)\n",
    "      \n",
    "     # ==== add trend ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] + ((a * i) + b)\n",
    "    \n",
    "        for i in range(0, fh):\n",
    "            Y_pred[i] = Y_pred[i] + ((a * (len(Data_val) + i + 1)) + b)\n",
    "           \n",
    "    # ==== add seasonality ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * seasonality_in[i % freq] / 100\n",
    "    \n",
    "        for i in range(len(Data_val), len(Data_val) + fh):\n",
    "            Y_pred[i - len(Data_val)] = Y_pred[i - len(Data_val)] * seasonality_in[i % freq] / 100\n",
    "        \n",
    "        #print(\"y_pre\",Y_pred)\n",
    "        #print(Data_val.head)\n",
    "    \n",
    "        # check the prediction on negativity and extremity\n",
    "        check_pred(Data_val,Y_pred)\n",
    "        \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        # calculation of Error\n",
    "        print(\" Time Serie nr.\",c, \" sMape: \",smape(y_test, Y_pred), \"     MAsE: \",mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        # calculation of Error\n",
    "        model_sMAPE.append(smape(y_test, Y_pred))\n",
    "        model_MASE.append(mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        c+=1\n",
    "    print(\"\\n\",\" Anzahl der Serie: \",c)\n",
    "    print(\"sMAPE:  \",np.mean(model_sMAPE))\n",
    "    print(\"MAsE:  \",np.mean(model_MASE))\n",
    "   \n",
    "    return np.mean(model_sMAPE), np.mean(model_MASE)\n",
    "\n",
    "def main_prediction_mlp (Data: pd.DataFrame,model,fh,freq): \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset  \n",
    "    model_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    model_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    # Iteration through each serie in the dataset\n",
    "    c=0\n",
    "    for i in range(n):\n",
    "        zr = Data.iloc[:,i]  \n",
    "        # remove all NaN value from the serie\n",
    "        new_Data = remov_nan (zr)        \n",
    "        in_back = int(0.2*len(new_Data))\n",
    "        # load the value of new_Data\n",
    "        Data_val = new_Data.values        \n",
    "        # ==== remove seasonality ====#\n",
    "        seasonality_in = deseasonalize(Data_val, freq)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * 100 / seasonality_in[i % freq]\n",
    "        # ==== detrending ====#\n",
    "        a, b = detrend(Data_val)\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] - ((a * i) + b)       \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        predict =[]\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "      # Techniques of Iteration for the horizon forcasting\n",
    "        for i in range(0, fh):\n",
    "            # add the first prediction to y_predict\n",
    "            predict.append(prediction_current)\n",
    "            # move the first element in x_test to the last position, in order to remove \n",
    "            x_test[0] = np.roll(x_test[0], -1)\n",
    "            # set now the current_prediction value at the last position of x_test\n",
    "            x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "            prediction_current = model.predict(x_test)[0]\n",
    "        Y_pred = np.asarray(predict)     \n",
    "     # ==== add trend ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] + ((a * i) + b)\n",
    "    \n",
    "        for i in range(0, fh):\n",
    "            Y_pred[i] = Y_pred[i] + ((a * (len(Data_val) + i + 1)) + b)\n",
    "           \n",
    "    # ==== add seasonality ====#\n",
    "        for i in range(0, len(Data_val)):\n",
    "            Data_val[i] = Data_val[i] * seasonality_in[i % freq] / 100\n",
    "    \n",
    "        for i in range(len(Data_val), len(Data_val) + fh):\n",
    "            Y_pred[i - len(Data_val)] = Y_pred[i - len(Data_val)] * seasonality_in[i % freq] / 100\n",
    "        \n",
    "        \n",
    "        # check the prediction on negativity and extremity\n",
    "        check_pred(new_Data,Y_pred)\n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        # calculation of Error\n",
    "        print(\" Time Serie nr.\",c, \" sMape: \",smape(y_test, Y_pred), \"     MAsE: \",mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        # calculation of Error\n",
    "        model_sMAPE.append(smape(y_test, Y_pred))\n",
    "        model_MASE.append(mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        c+=1\n",
    "    print(\"\\n\",\" Anzahl der Serie: \",c)\n",
    "    print(\"sMAPE:  \",np.mean(model_sMAPE))\n",
    "    print(\"MAsE:  \",np.mean(model_MASE))\n",
    "    print(\"\\n\")\n",
    "    return np.mean(model_sMAPE), np.mean(model_MASE)\n",
    "\n",
    "def main_prediction_SVM (Data: pd.DataFrame,model,fh,freq):\n",
    " \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset \n",
    "    model_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    model_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    # Iteration through each serie in the dataset\n",
    "    c=0\n",
    "    for i in range(n):\n",
    "        zr = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        new_Data = remov_nan (zr)\n",
    "        \n",
    "        in_back = int(0.2*len(new_Data))\n",
    "        # load the value of new_Data\n",
    "        Data_val = new_Data.values\n",
    "        \n",
    "        rr ,s = normalisieren_data(Data_val.reshape(-1, 1))\n",
    "        Data_val_norm = np.reshape(rr,len(Data_val))\n",
    "        \n",
    "        \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val_norm,fh, in_back)\n",
    "        \n",
    "        model.fit(x_train,y_train)\n",
    "        \n",
    "        predict =[]\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "        # Techniques of Iteration for the horizon forcasting\n",
    "        for i in range(0, fh):\n",
    "            # add the first prediction to y_predict\n",
    "            predict.append(prediction_current)\n",
    "            # move the first element in x_test to the last position, in order to remove \n",
    "            x_test[0] = np.roll(x_test[0], -1)\n",
    "            # set now the current_prediction value at the last position of x_test\n",
    "            x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "            prediction_current = model.predict(x_test)[0]\n",
    "        Y_pred_LinReg = np.asarray(predict) \n",
    "        Y_pred = s.inverse_transform([Y_pred_LinReg])[0]\n",
    "        \n",
    "        # check the prediction on negativity and extremity\n",
    "        check_pred(new_Data,Y_pred)\n",
    "        \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        # calculation of Error\n",
    "        print(\" Time Serie nr.\",c, \" sMape: \",smape(y_test, Y_pred), \"     MAsE: \",mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        # calculation of Error\n",
    "        model_sMAPE.append(smape(y_test, Y_pred))\n",
    "        model_MASE.append(mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        c+=1\n",
    "    print(\"\\n\",\" Anzahl der Serie: \",c)\n",
    "    print(\"sMAPE:  \",np.mean(model_sMAPE))\n",
    "    print(\"MAsE:  \",np.mean(model_MASE))\n",
    "   \n",
    "    return np.mean(model_sMAPE), np.mean(model_MASE)\n",
    "\n",
    "def main_prediction(Data: pd.DataFrame,model,fh,freq):   \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset  \n",
    "    model_MASE =[]     # a list to save all mase_values of model of each Serie forcasting \n",
    "    model_sMAPE =[]    # a list to save all smape_values of model of each Serie forcasting\n",
    "    c=0\n",
    "    # Iteration through each serie in the dataset\n",
    "    for i in range(n):\n",
    "        zr = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        new_Data = remov_nan (zr)\n",
    "        \n",
    "        in_back = int(0.2*len(new_Data))\n",
    "        # load the value of new_Data\n",
    "        Data_val = new_Data.values\n",
    "        \n",
    "        x_train,y_train,x_test,y_test = all_split(Data_val,fh, in_back)\n",
    "        \n",
    "        model.fit(x_train,y_train)\n",
    "        \n",
    "        predict =[]\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "        # Techniques of Iteration for the horizon forcasting\n",
    "        for i in range(0, fh):\n",
    "            # add the first prediction to y_predict\n",
    "            predict.append(prediction_current)\n",
    "            # move the first element in x_test to the last position, in order to remove \n",
    "            x_test[0] = np.roll(x_test[0], -1)\n",
    "            # set now the current_prediction value at the last position of x_test\n",
    "            x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "            prediction_current = model.predict(x_test)[0]\n",
    "        Y_pred = np.asarray(predict)  \n",
    "        # check the prediction on negativity and extremity\n",
    "        check_pred(new_Data,Y_pred)       \n",
    "        # calculation of Error\n",
    "        print(\" Time Serie nr.\",c, \" sMape: \",smape(y_test, Y_pred), \"     MAsE: \",mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        # calculation of Error\n",
    "        model_sMAPE.append(smape(y_test, Y_pred))\n",
    "        model_MASE.append(mase(Data_val[:-fh], y_test, Y_pred, freq))\n",
    "        c+=1\n",
    "    print(\"\\n\",\" Anzahl der Serie: \",c)\n",
    "    print(\"sMAPE:  \",np.mean(model_sMAPE))\n",
    "    print(\"MAsE:  \",np.mean(model_MASE))\n",
    "   \n",
    "    return np.mean(model_sMAPE), np.mean(model_MASE)\n",
    "\n",
    "def main_prediction_all_data(model):\n",
    "            T_sMape = []\n",
    "            T_Mase = []\n",
    "    # Hourly Daten\n",
    "            print('Dataset_hourly')\n",
    "\n",
    "            sMape_hourly_general,Mase_hourly_general = main_prediction(Dataset_hourly,model,48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten    \n",
    "            print('Daily daten')\n",
    "            sMape_daily_general, Mase_daily_general = main_prediction(Dataset_daily,model,14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general) \n",
    "            \n",
    "    # Weekly daten\n",
    "            print('Weekly daten')\n",
    "            sMape_weekly_general,Mase_weekly_general = main_prediction(Dataset_weekly,model,13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)   \n",
    "    # Monthly Daten\n",
    "            print('Monthly daten')\n",
    "            sMape_monthly_general,Mase_monthly_general= main_prediction(Dataset_monthly,model,18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general) \n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_prediction(Dataset_quaterly,model,8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general)\n",
    "    # Yearly Daten\n",
    "            print('Yearly daten')\n",
    "            sMape_yearly_general,Mase_yearly_general =main_prediction(Dataset_yearly,model,6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            return p\n",
    "\n",
    "\n",
    "\n",
    "def global_prediction():\n",
    "    \n",
    "    c = 0\n",
    "    m = []\n",
    "    m.append(Lin_Reg())\n",
    "    m.append(Dtree_Regression())\n",
    "    m.append(K_neaRegression())\n",
    "    #m.append(Ridge_Regression())\n",
    "    m.append(SVM_Regression())\n",
    "    m.append(mlp_benchm())\n",
    "    m.append(\"rnn_bench\")\n",
    "    m.append(\"LSTM_NN\")\n",
    "    b = np.array([])\n",
    "    columnsname= [\"Model\",\"sMape Hourly\",\"sMape Daily\",\"sMape Weekly\",\"sMape Monthly\",\"sMape Quaterly\",\"sMape Yearly\",\"sMAPE TOTAL\",\n",
    "                 \"Mase Hourly\",\"Mase Daily\",\"Mase Weekly\",\"Mase Monthly\",\"Mase Quaterly\",\"Mase Yearly\",\"MAPE TOTAL\", ]\n",
    "    ds = pd.DataFrame(columns=columnsname )\n",
    "    ds.to_csv('out.csv')\n",
    "\n",
    "    for i in range (len(m)):\n",
    "        print(i)\n",
    "        if i<3:\n",
    "            T_sMape = []\n",
    "            T_Mase = []\n",
    "    # Hourly Daten\n",
    "            print('Dataset_hourly')\n",
    "\n",
    "            sMape_hourly_general,Mase_hourly_general = main_prediction(Dataset_hourly,m[i],48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten    \n",
    "            print('Daily daten')\n",
    "            sMape_daily_general, Mase_daily_general = main_prediction(Dataset_daily,m[i],14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general) \n",
    "            \n",
    "    # Weekly daten\n",
    "            print('Weekly daten')\n",
    "            sMape_weekly_general,Mase_weekly_general = main_prediction(Dataset_weekly,m[i],13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)   \n",
    "    # Monthly Daten\n",
    "            print('Monthly daten')\n",
    "            sMape_monthly_general,Mase_monthly_general= main_prediction(Dataset_monthly,m[i],18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general) \n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_prediction(Dataset_quaterly,m[i],8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general)\n",
    "    # Yearly Daten\n",
    "            print('Yearly daten')\n",
    "            sMape_yearly_general,Mase_yearly_general =main_prediction(Dataset_yearly,m[i],6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            pprint(p)\n",
    "            ds.loc[i] = p        \n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('out.csv', mode='a', header=False)\n",
    "\n",
    "        ########################################################################################################\n",
    "        \n",
    "        \n",
    "        if i == 3:                     ## Schleife von support vector Regression\n",
    "            T_sMape = []\n",
    "            T_Mase = [] \n",
    "     # Hourly Daten\n",
    "            sMape_hourly_general,Mase_hourly_general = main_prediction_SVM(Dataset_hourly,m[i],48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten  \n",
    "            sMape_daily_general, Mase_daily_general = main_prediction_SVM(Dataset_daily,m[i],14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general)   \n",
    "    # Weekly daten\n",
    "            sMape_weekly_general,Mase_weekly_general = main_prediction_SVM(Dataset_weekly,m[i],13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)  \n",
    "    # Monthly Daten\n",
    "            sMape_monthly_general,Mase_monthly_general= main_prediction_SVM(Dataset_monthly,m[i],18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general)    \n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_prediction_SVM(Dataset_quaterly,m[i],8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general)\n",
    "    # Yearly Daten\n",
    "            sMape_yearly_general,Mase_yearly_general =main_prediction_SVM(Dataset_yearly,m[i],6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    " \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            pprint(p)\n",
    "            ds.loc[i] = p        \n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('out.csv', mode='a', header=False)\n",
    "      \n",
    "    ###############################################################################################################\n",
    "    \n",
    "        if i == 4:                     ## Schleife von mlp Bench\n",
    "            T_sMape = []\n",
    "            T_Mase = [] \n",
    "     # Hourly Daten\n",
    "            sMape_hourly_general,Mase_hourly_general = main_prediction_mlp(Dataset_hourly,m[i],48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten  \n",
    "            sMape_daily_general, Mase_daily_general = main_prediction_mlp(Dataset_daily,m[i],14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general)   \n",
    "    # Weekly daten\n",
    "            sMape_weekly_general,Mase_weekly_general = main_prediction_mlp(Dataset_weekly,m[i],13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)  \n",
    "    # Monthly Daten\n",
    "            sMape_monthly_general,Mase_monthly_general= main_prediction_mlp(Dataset_monthly,m[i],18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general)    \n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_prediction_mlp(Dataset_quaterly,m[i],8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general)\n",
    "    # Yearly Daten\n",
    "            sMape_yearly_general,Mase_yearly_general =main_prediction_mlp(Dataset_yearly,m[i],6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            pprint(p)\n",
    "            ds.loc[i] = p        \n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('out.csv', mode='a', header=False)\n",
    "      \n",
    "        #print(.shape)\n",
    "       \n",
    "    ###########################################################################################################\n",
    "    \n",
    "        if i == 5:                     ## Schleife von RNN Bench\n",
    "            T_sMape = []\n",
    "            T_Mase = [] \n",
    "     # Hourly Daten\n",
    "            sMape_hourly_general,Mase_hourly_general = main_predict_rnn(Dataset_hourly,48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten\n",
    "            sMape_daily_general, Mase_daily_general = main_predict_rnn(Dataset_daily,14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general)  \n",
    "    # Weekly daten\n",
    "            sMape_weekly_general,Mase_weekly_general = main_predict_rnn(Dataset_weekly.iloc[:,0:2],13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)  \n",
    "    # Monthly Daten\n",
    "            sMape_monthly_general,Mase_monthly_general= main_predict_rnn(Dataset_monthly,18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general)\n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_predict_rnn(Dataset_quaterly,8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general)\n",
    "    # Yearly Daten\n",
    "            sMape_yearly_general,Mase_yearly_general =main_predict_rnn(Dataset_yearly,6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            pprint(p)\n",
    "            ds.loc[i] = p        \n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('out.csv', mode='a', header=False)\n",
    "      \n",
    "        \n",
    "        ########################################################################################################\n",
    "        \n",
    "        if i == 6:                ## Schleife von LSTM_NN\n",
    "            T_sMape = []\n",
    "            T_Mase = [] \n",
    "     # Hourly Daten\n",
    "            sMape_hourly_general,Mase_hourly_general = main_predict_lstm(Dataset_hourly,48,24)\n",
    "            T_sMape.append(sMape_hourly_general)\n",
    "            T_Mase.append(Mase_hourly_general)\n",
    "     # Daily daten    \n",
    "            sMape_daily_general, Mase_daily_general = main_predict_lstm(Dataset_daily,14,1)\n",
    "            T_sMape.append(sMape_daily_general)\n",
    "            T_Mase.append(Mase_daily_general)   \n",
    "    # Weekly daten\n",
    "            sMape_weekly_general,Mase_weekly_general = main_predict_lstm(Dataset_weekly,13,1)\n",
    "            T_sMape.append(sMape_weekly_general)\n",
    "            T_Mase.append(Mase_weekly_general)    \n",
    "    # Monthly Daten\n",
    "            sMape_monthly_general,Mase_monthly_general= main_predict_lstm(Dataset_monthly,18,12)\n",
    "            T_sMape.append(sMape_monthly_general)\n",
    "            T_Mase.append(Mase_monthly_general)  \n",
    "    # Quaterly Daten\n",
    "            sMape_quaterly_general,Mase_quaterly_general= main_predict_lstm(Dataset_quaterly,8,4)\n",
    "            T_sMape.append(sMape_quaterly_general)\n",
    "            T_Mase.append(Mase_quaterly_general) \n",
    "    # Yearly Daten\n",
    "            sMape_yearly_general,Mase_yearly_general =main_predict_lstm(Dataset_yearly,6,1)\n",
    "            T_sMape.append(sMape_yearly_general)\n",
    "            T_Mase.append(Mase_yearly_general)\n",
    "  \n",
    "            p = [m[i],sMape_hourly_general,sMape_daily_general,sMape_weekly_general,\n",
    "                   sMape_monthly_general,sMape_quaterly_general,sMape_yearly_general,np.mean(T_sMape),\n",
    "                   Mase_hourly_general,Mase_daily_general,Mase_weekly_general,\n",
    "                   Mase_monthly_general,Mase_quaterly_general,Mase_yearly_general,np.mean(T_Mase)\n",
    "                  ]\n",
    "            pprint(p)\n",
    "            ds.loc[i] = p        \n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('out.csv', mode='a', header=False)\n",
    "      \n",
    "        \n",
    "    ################################################################################\n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

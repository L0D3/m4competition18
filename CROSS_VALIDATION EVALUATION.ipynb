{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/jupyterNotebooks/m4competition18/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Laden von der verschieden Bibliothek zur Daten Visualizierung und Vorhersagen\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression ,BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.optimizers import rmsprop\n",
    "from keras import backend as ker\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "os.chdir('/home/jupyter/jupyterNotebooks/m4competition18/data')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:   (960, 414) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hourly = pd.read_csv(\"Hourly-train.csv\", skiprows=0, index_col =0)\n",
    "\n",
    "\n",
    "\n",
    "Dataset_hourly = df_hourly.T.iloc[:,:]\n",
    "print (\"Dimension:  \",Dataset_hourly.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:   (2597, 359) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Laden von Dataset Weekly-train\n",
    "df_weekly = pd.read_csv(\"Weekly-train.csv\", skiprows=0, index_col =0)\n",
    "\n",
    "\n",
    "Dataset_weekly = df_weekly.T\n",
    "Weekly_train_Finance = df_weekly[59:223].T.iloc[:,:]\n",
    "Weekly_train_Other = df_weekly[:12].T.iloc[:,:]\n",
    "Weekly_train_Industry = df_weekly[53:59].T.iloc[:,:]\n",
    "Weekly_train_Demographic = df_weekly[223:247].T.iloc[:,:]\n",
    "Weekly_train_Micro = df_weekly[247:359].T.iloc[:,:]\n",
    "Weekly_train_Macro = df_weekly[12:53].T.iloc[:,:]\n",
    "\n",
    "print (\"Dimension:  \",Dataset_weekly.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laden von  Dataset Yearly-train\n",
    "df_yearly = pd.read_csv(\"Yearly-train.csv\", skiprows=0, index_col =0)\n",
    "\n",
    "\n",
    "Dataset_yearly = df_yearly.T.iloc[:,:]\n",
    "print (\"Dimension:  \",Dataset_yearly.shape,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:   (2794, 48000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Laden von Dataset Monthly-train\n",
    "df_monthly = pd.read_csv(\"Monthly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_monthly = df_monthly.T.iloc[:,:]\n",
    "print (\"Dimension:  \",Dataset_monthly.shape,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laden von Dataset Quarterly-train\n",
    "df_quaterly = pd.read_csv(\"Quarterly-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_quaterly = df_quaterly.T.iloc[:,:]\n",
    "print (\"Dimension:  \",Dataset_quaterly.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = pd.read_csv(\"Daily-train.csv\", skiprows=0, index_col =0)\n",
    "Dataset_daily = df_daily.T.iloc[:,:]\n",
    "print (\"Dimension:  \",Dataset_daily.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remov_nan (dataset):\n",
    "    '''\n",
    "    to remove all NaN Values in a \n",
    "    Time Serie Dataframe\n",
    "    '''\n",
    "    n = dataset.isnull().sum() \n",
    "    data = dataset[0:(len(dataset)-n)]\n",
    "    return data\n",
    "\n",
    "def copy_val(x):\n",
    "    '''\n",
    "    to copy a list or array in a new memory \n",
    "    without reference \n",
    "    x: list or array\n",
    "    '''\n",
    "    y =[]\n",
    "    for i in x:\n",
    "        y.append(i)\n",
    "    return np.array(y)\n",
    "## BENCHMARK ##\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item() \n",
    "\n",
    "##===Mean Absolute Scaled Error ====##\n",
    "def mase(insample, y_test, y_hat_test, freq):\n",
    "    \"\"\"\n",
    "    Calculates MAsE\n",
    "    :param insample: insample data\n",
    "    :param y_test: out of sample target values\n",
    "    :param y_hat_test: predicted values\n",
    "    :param freq: data frequency\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_naive = []\n",
    "    for i in range(freq, len(insample)):\n",
    "        y_hat_naive.append(insample[(i - freq)])\n",
    "    masep = np.mean(abs(insample[freq:] - y_hat_naive))\n",
    "    return np.mean(abs(y_test - y_hat_test)) / masep\n",
    "#Hier wird die \"Time serie\" als \"spervised learning Problem\" umgewandel.\n",
    "#Die Datenmenge der Zeitreihen wird in Training und Testing Datamenge und jeweils in input & output Daten\n",
    "\n",
    "# Hilfsfunktion , die eine Datenmenge in input und output Menge aufteile \n",
    "def split_input_output(dataset: np.ndarray, in_back: int=1) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" \n",
    "    The function takes two arguments: the `dataset`, which is a NumPy array that we want to convert into a dataset,\n",
    "    and the `in_back`, which is the number of previous time steps to use as input variables\n",
    "    to predict the next time period — in this case defaulted to 1.\n",
    "    :dataset: numpy dataset\n",
    "    :in_variable: number of previous time steps as int\n",
    "    :return: tuple of input and output dataset\n",
    "    \"\"\"\n",
    "    Input, Output = [], []\n",
    "    for i in range(len(dataset)-in_back):\n",
    "        a = dataset[i:(i+in_back)]\n",
    "        Input.append(a)\n",
    "        Output.append(dataset[i + in_back])\n",
    "    return np.array(Input), np.array(Output)\n",
    "\n",
    "## Folgende Funktion split die Datenmende in Training and Testing Daten.\n",
    "\n",
    "def split_into_train_test(dataset: np.ndarray,train_size, in_back) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test datasets. \n",
    "    : dataset: (np.ndarray) Time serie Dataset \n",
    "    : train_size: (int) Größe der Training Datamenge\n",
    "    : look_back: (int) number of previous time steps \n",
    "    :return: tuple of training data and test dataset\n",
    "    \"\"\"\n",
    "    if not train_size > in_back:\n",
    "        raise ValueError('train_size muss größer als look_back',\"train_size:\",train_size,\"in_back:\",in_back)\n",
    "    train= dataset[0:train_size]\n",
    "    test = dataset[train_size - in_back:len(dataset)]\n",
    "    #print('train_dataset: {}, test_dataset: {}'.format(len(train), len(test)))\n",
    "    return train, test\n",
    "\n",
    "## Es wird hier die Datenmenge in X_train,Y_train für das Training und X_test,Y_test für das Testing \n",
    "\n",
    "def all_split (dataset: np.ndarray,fh, in_back) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits dataset into input-training (X_train), outout_training(Y_train) and input_test(X_test) , output_test(Y_test) datasets.\n",
    "    : dataset:(np.ndarray) Time serie Dataset\n",
    "    :df:(float64) Größe der Testing Datamenge \n",
    "    : in_back: (int) number of previous time steps \n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    #if not (size_prozent>0 and size_prozent<1):\n",
    "        #raise ValueError('size_prozent of training must be in the interval 0 and 1')\n",
    "    train_size = len(dataset)-fh\n",
    "    training, testing = split_into_train_test(dataset,train_size,in_back)\n",
    "    X_train, Y_train = split_input_output(training,in_back)\n",
    "    X_test, Y_test = split_input_output(testing,in_back)\n",
    "    return X_train,Y_train,X_test[0].reshape(1,-1),Y_test\n",
    "\n",
    "def check_pred (dataset: pd.DataFrame,y_pred: np.ndarray):\n",
    "    ''''\n",
    "    this function check the negativity of the predicted values, set them to null \n",
    "    if they are negativ and to max value of the serie data if they are extrem high\n",
    "    : dataset: Dataset of the serie\n",
    "    : y_pred:  The list of predicted values\n",
    "    : return:\n",
    "    '''\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0:\n",
    "            y_pred[i]=0\n",
    "        if y_pred[i]> (9000*max(dataset)):\n",
    "            y_pred[i]=max(dataset)\n",
    "            \n",
    "\n",
    "def LinReg_mod(x_train, y_train, x_test,forcast_horizon):\n",
    "    '''\n",
    "    This function return the prediction of x_test with \n",
    "    the model 'linear Regression'. The parameter 'normalize' of the model will be set on True\n",
    "    : x_train: Input Training Dataset\n",
    "    : y_train: Outpout Training Dataset\n",
    "    : x_test: Input Testing Dataset\n",
    "    : forcast_horizon: the horizon of forcasting\n",
    "    '''\n",
    "    y_predict = []\n",
    "    # load of the model and setting of parameters\n",
    "    model = LinearRegression(normalize=True)   #ARDRegression(compute_score=True)\n",
    "    # Training of the model with x_train and y_train\n",
    "    model.fit(x_train,y_train)\n",
    "    # prediction of the model\n",
    "    prediction_current = model.predict(x_test)[0]\n",
    "    # Techniques of Iteration for the horizon forcasting\n",
    "    for i in range(0, forcast_horizon):\n",
    "        # add the first prediction to y_predict\n",
    "        y_predict.append(prediction_current)\n",
    "        # move the first element in x_test to the last position, in order to remove \n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        # set now the current_prediction value at the last position of x_test\n",
    "        x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "    return  np.asarray(y_predict) \n",
    "\n",
    "def Dtree_Reg  (x_train, y_train, x_test, fh):\n",
    "    '''\n",
    "    This function return the prediction of x_test with \n",
    "    the model 'Decision Tree Regression'. \n",
    "    : x_train: Input Training Dataset\n",
    "    : y_train: Outpout Training Dataset\n",
    "    : x_test: Input Testing Dataset\n",
    "    : fh: the horizon of forcasting\n",
    "    '''\n",
    "    y_predict = []\n",
    "    # load of the model and setting of parameters\n",
    "    model = DecisionTreeRegressor(criterion='mae',max_depth=28,min_samples_split=5,\n",
    "                                 min_samples_leaf =5)\n",
    "    # Training of the model with x_train and y_train\n",
    "    model.fit(x_train,y_train)\n",
    "    # prediction of the model\n",
    "    prediction_current = model.predict(x_test)[0]\n",
    "    # Techniques of Iteration for the horizon forcasting\n",
    "    for i in range(0, fh):\n",
    "        # add the first prediction to y_predict\n",
    "        y_predict.append(prediction_current)\n",
    "        # move the first element in x_test to the last position, in order to remove \n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        # set now the current_prediction value at the last position of x_test\n",
    "        x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "    return  np.asarray(y_predict) \n",
    "\n",
    "def K_neaReg (x_train, y_train, x_test, fh):\n",
    "    \n",
    "    '''\n",
    "    This function return the prediction of x_test with \n",
    "    the model 'k-nearest neighbours'. \n",
    "    : x_train: Input Training Dataset\n",
    "    : y_train: Outpout Training Dataset\n",
    "    : x_test: Input Testing Dataset\n",
    "    : forcast_horizon: the horizon of forcasting\n",
    "    '''\n",
    "    \n",
    "    y_predict = []\n",
    "    # load of the model and setting of parameters\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    # Training of the model with x_train and y_train\n",
    "    model.fit(x_train,y_train)\n",
    "    # prediction of the model\n",
    "    prediction_current = model.predict(x_test)[0]\n",
    "    # Techniques of Iteration for the horizon forcasting\n",
    "    for i in range(0, fh):\n",
    "        # add the first prediction to y_predict\n",
    "        y_predict.append(prediction_current)\n",
    "        # move the first element in x_test to the last position, in order to remove \n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        # set now the current_prediction value at the last position of x_test\n",
    "        x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "    return  np.asarray(y_predict) \n",
    "\n",
    "def SVM_Reg (x_train, y_train, x_test, fh):\n",
    "    \n",
    "    '''\n",
    "    This function return the prediction of x_test with \n",
    "    the model 'Support Vector Regression'. \n",
    "    : x_train: Input Training Dataset\n",
    "    : y_train: Outpout Training Dataset\n",
    "    : x_test: Input Testing Dataset\n",
    "    : fh: the horizon of forcasting\n",
    "    '''\n",
    "    y_predict = []\n",
    "    # load of the model and setting of parameters\n",
    "    model =  SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    # Training of the model with x_train and y_train\n",
    "    model.fit(x_train,y_train)\n",
    "    # prediction of the model\n",
    "    prediction_current = model.predict(x_test)[0]\n",
    "    # Techniques of Iteration for the horizon forcasting\n",
    "    for i in range(0, fh):\n",
    "        # add the first prediction to y_predict\n",
    "        y_predict.append(prediction_current)\n",
    "        # move the first element in x_test to the last position, in order to remove \n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        # set now the current_prediction value at the last position of x_test\n",
    "        x_test[0][(len(x_test[0]) - 1)] = prediction_current\n",
    "        prediction_current = model.predict(x_test)[0]\n",
    "    return  np.asarray(y_predict)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Nr: 1\n",
      "Anzahl Observations:  133\n",
      "train Observations:  120\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 2\n",
      "Anzahl Observations:  146\n",
      "train Observations:  133\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 3\n",
      "Anzahl Observations:  159\n",
      "train Observations:  146\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 4\n",
      "Anzahl Observations:  172\n",
      "train Observations:  159\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 5\n",
      "Anzahl Observations:  185\n",
      "train Observations:  172\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 6\n",
      "Anzahl Observations:  198\n",
      "train Observations:  185\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 7\n",
      "Anzahl Observations:  211\n",
      "train Observations:  198\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 8\n",
      "Anzahl Observations:  224\n",
      "train Observations:  211\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 9\n",
      "Anzahl Observations:  237\n",
      "train Observations:  224\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 10\n",
      "Anzahl Observations:  250\n",
      "train Observations:  237\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 11\n",
      "Anzahl Observations:  263\n",
      "train Observations:  250\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 12\n",
      "Anzahl Observations:  276\n",
      "train Observations:  263\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 13\n",
      "Anzahl Observations:  289\n",
      "train Observations:  276\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 14\n",
      "Anzahl Observations:  302\n",
      "train Observations:  289\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 15\n",
      "Anzahl Observations:  315\n",
      "train Observations:  302\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 16\n",
      "Anzahl Observations:  328\n",
      "train Observations:  315\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 17\n",
      "Anzahl Observations:  341\n",
      "train Observations:  328\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 18\n",
      "Anzahl Observations:  354\n",
      "train Observations:  341\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 19\n",
      "Anzahl Observations:  367\n",
      "train Observations:  354\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 20\n",
      "Anzahl Observations:  380\n",
      "train Observations:  367\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 21\n",
      "Anzahl Observations:  393\n",
      "train Observations:  380\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 22\n",
      "Anzahl Observations:  406\n",
      "train Observations:  393\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 23\n",
      "Anzahl Observations:  419\n",
      "train Observations:  406\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 24\n",
      "Anzahl Observations:  432\n",
      "train Observations:  419\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 25\n",
      "Anzahl Observations:  445\n",
      "train Observations:  432\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 26\n",
      "Anzahl Observations:  458\n",
      "train Observations:  445\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 27\n",
      "Anzahl Observations:  471\n",
      "train Observations:  458\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 28\n",
      "Anzahl Observations:  484\n",
      "train Observations:  471\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 29\n",
      "Anzahl Observations:  497\n",
      "train Observations:  484\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 30\n",
      "Anzahl Observations:  510\n",
      "train Observations:  497\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 31\n",
      "Anzahl Observations:  523\n",
      "train Observations:  510\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 32\n",
      "Anzahl Observations:  536\n",
      "train Observations:  523\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 33\n",
      "Anzahl Observations:  549\n",
      "train Observations:  536\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 34\n",
      "Anzahl Observations:  562\n",
      "train Observations:  549\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 35\n",
      "Anzahl Observations:  575\n",
      "train Observations:  562\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 36\n",
      "Anzahl Observations:  588\n",
      "train Observations:  575\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 37\n",
      "Anzahl Observations:  601\n",
      "train Observations:  588\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 38\n",
      "Anzahl Observations:  614\n",
      "train Observations:  601\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 39\n",
      "Anzahl Observations:  627\n",
      "train Observations:  614\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 40\n",
      "Anzahl Observations:  640\n",
      "train Observations:  627\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 41\n",
      "Anzahl Observations:  653\n",
      "train Observations:  640\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 42\n",
      "Anzahl Observations:  666\n",
      "train Observations:  653\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 43\n",
      "Anzahl Observations:  679\n",
      "train Observations:  666\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 44\n",
      "Anzahl Observations:  692\n",
      "train Observations:  679\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 45\n",
      "Anzahl Observations:  705\n",
      "train Observations:  692\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 46\n",
      "Anzahl Observations:  718\n",
      "train Observations:  705\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 47\n",
      "Anzahl Observations:  731\n",
      "train Observations:  718\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 48\n",
      "Anzahl Observations:  744\n",
      "train Observations:  731\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 49\n",
      "Anzahl Observations:  757\n",
      "train Observations:  744\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 50\n",
      "Anzahl Observations:  770\n",
      "train Observations:  757\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 51\n",
      "Anzahl Observations:  783\n",
      "train Observations:  770\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 52\n",
      "Anzahl Observations:  796\n",
      "train Observations:  783\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 53\n",
      "Anzahl Observations:  809\n",
      "train Observations:  796\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 54\n",
      "Anzahl Observations:  822\n",
      "train Observations:  809\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 55\n",
      "Anzahl Observations:  835\n",
      "train Observations:  822\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 56\n",
      "Anzahl Observations:  848\n",
      "train Observations:  835\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 57\n",
      "Anzahl Observations:  861\n",
      "train Observations:  848\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 58\n",
      "Anzahl Observations:  874\n",
      "train Observations:  861\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 59\n",
      "Anzahl Observations:  887\n",
      "train Observations:  874\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 60\n",
      "Anzahl Observations:  900\n",
      "train Observations:  887\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 61\n",
      "Anzahl Observations:  913\n",
      "train Observations:  900\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 62\n",
      "Anzahl Observations:  926\n",
      "train Observations:  913\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 63\n",
      "Anzahl Observations:  939\n",
      "train Observations:  926\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 64\n",
      "Anzahl Observations:  952\n",
      "train Observations:  939\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 65\n",
      "Anzahl Observations:  965\n",
      "train Observations:  952\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 66\n",
      "Anzahl Observations:  978\n",
      "train Observations:  965\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 67\n",
      "Anzahl Observations:  991\n",
      "train Observations:  978\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 68\n",
      "Anzahl Observations:  1004\n",
      "train Observations:  991\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 69\n",
      "Anzahl Observations:  1017\n",
      "train Observations:  1004\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 70\n",
      "Anzahl Observations:  1030\n",
      "train Observations:  1017\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 71\n",
      "Anzahl Observations:  1043\n",
      "train Observations:  1030\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 72\n",
      "Anzahl Observations:  1056\n",
      "train Observations:  1043\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 73\n",
      "Anzahl Observations:  1069\n",
      "train Observations:  1056\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 74\n",
      "Anzahl Observations:  1082\n",
      "train Observations:  1069\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 75\n",
      "Anzahl Observations:  1095\n",
      "train Observations:  1082\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 76\n",
      "Anzahl Observations:  1108\n",
      "train Observations:  1095\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 77\n",
      "Anzahl Observations:  1121\n",
      "train Observations:  1108\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 78\n",
      "Anzahl Observations:  1134\n",
      "train Observations:  1121\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 79\n",
      "Anzahl Observations:  1147\n",
      "train Observations:  1134\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 80\n",
      "Anzahl Observations:  1160\n",
      "train Observations:  1147\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 81\n",
      "Anzahl Observations:  1173\n",
      "train Observations:  1160\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 82\n",
      "Anzahl Observations:  1186\n",
      "train Observations:  1173\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 83\n",
      "Anzahl Observations:  1199\n",
      "train Observations:  1186\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 84\n",
      "Anzahl Observations:  1212\n",
      "train Observations:  1199\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 85\n",
      "Anzahl Observations:  1225\n",
      "train Observations:  1212\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 86\n",
      "Anzahl Observations:  1238\n",
      "train Observations:  1225\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 87\n",
      "Anzahl Observations:  1251\n",
      "train Observations:  1238\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 88\n",
      "Anzahl Observations:  1264\n",
      "train Observations:  1251\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 89\n",
      "Anzahl Observations:  1277\n",
      "train Observations:  1264\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 90\n",
      "Anzahl Observations:  1290\n",
      "train Observations:  1277\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 91\n",
      "Anzahl Observations:  1303\n",
      "train Observations:  1290\n",
      "test Observations:  13 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Nr: 92\n",
      "Anzahl Observations:  1316\n",
      "train Observations:  1303\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 93\n",
      "Anzahl Observations:  1329\n",
      "train Observations:  1316\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 94\n",
      "Anzahl Observations:  1342\n",
      "train Observations:  1329\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 95\n",
      "Anzahl Observations:  1355\n",
      "train Observations:  1342\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 96\n",
      "Anzahl Observations:  1368\n",
      "train Observations:  1355\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 97\n",
      "Anzahl Observations:  1381\n",
      "train Observations:  1368\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 98\n",
      "Anzahl Observations:  1394\n",
      "train Observations:  1381\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 99\n",
      "Anzahl Observations:  1407\n",
      "train Observations:  1394\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 100\n",
      "Anzahl Observations:  1420\n",
      "train Observations:  1407\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 101\n",
      "Anzahl Observations:  1433\n",
      "train Observations:  1420\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 102\n",
      "Anzahl Observations:  1446\n",
      "train Observations:  1433\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 103\n",
      "Anzahl Observations:  1459\n",
      "train Observations:  1446\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 104\n",
      "Anzahl Observations:  1472\n",
      "train Observations:  1459\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 105\n",
      "Anzahl Observations:  1485\n",
      "train Observations:  1472\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 106\n",
      "Anzahl Observations:  1498\n",
      "train Observations:  1485\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 107\n",
      "Anzahl Observations:  1511\n",
      "train Observations:  1498\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 108\n",
      "Anzahl Observations:  1524\n",
      "train Observations:  1511\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 109\n",
      "Anzahl Observations:  1537\n",
      "train Observations:  1524\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 110\n",
      "Anzahl Observations:  1550\n",
      "train Observations:  1537\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 111\n",
      "Anzahl Observations:  1563\n",
      "train Observations:  1550\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 112\n",
      "Anzahl Observations:  1576\n",
      "train Observations:  1563\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 113\n",
      "Anzahl Observations:  1589\n",
      "train Observations:  1576\n",
      "test Observations:  13 \n",
      "\n",
      "\n",
      "Split Nr: 114\n",
      "Anzahl Observations:  1602\n",
      "train Observations:  1589\n",
      "test Observations:  13 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6292404861275234, 0.3398820902333456)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def CV_Evaluation (dataset: pd.DataFrame,fh,in_back,freq):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # number of Split\n",
    "    f = int((dataset.shape[0]/(fh+1))) \n",
    "    \n",
    "    # check of the number of split\n",
    "    if f <= 1:\n",
    "        raise ValueError(\"Split is 1 or 0\",f)\n",
    "    \n",
    "    # Time Series cross-validator of sckitlearn\n",
    "    splits = TimeSeriesSplit(n_splits=f)\n",
    "    \n",
    "    index = 1\n",
    "\n",
    "    MASE = []    # a list to save all mase_value of each Split\n",
    "    sMAPE = []    # a list to save all smape_value of each Split\n",
    "\n",
    "    # Iteration that generates the index of each split\n",
    "    for train_index, test_index in splits.split(dataset):\n",
    "        \n",
    "        # The merge of the generate Split\n",
    "        train = dataset[train_index]\n",
    "        test = dataset[test_index]\n",
    "        frames = [train,test]     \n",
    "        SPLIT = pd.concat(frames)\n",
    "        print (\"Split Nr:\", index)\n",
    "        print(\"Anzahl Observations: \", len(train)+len(test))\n",
    "        print(\"train Observations: \", len(train))\n",
    "        print(\"test Observations: \", len(test),\"\\n\\n\")\n",
    "        # the Split will be splitted into x_train,y_train,x_test,y_test\n",
    "        x_train,y_train,x_test,y_test =  all_split(SPLIT,fh,in_back)\n",
    "        \n",
    "        #Prediction of the model\n",
    "        Y_pred_LinReg = Dtree_Reg(x_train, y_train, copy_val(x_test),fh)\n",
    "        \n",
    "        # Check of the \n",
    "        check_pred(dataset,Y_pred_LinReg)\n",
    "        \n",
    "        # The value of the metrics will be calculated and added to the list\n",
    "        sMAPE.append(smape(y_test, Y_pred_LinReg))\n",
    "        MASE.append(mase(train, y_test, Y_pred_LinReg, 1))    \n",
    "        \n",
    "        index += 1\n",
    "     \n",
    "    #print(\"MASE:  \",np.mean(MASE))    \n",
    "    #print(\"sMAPE:  \",np.mean(sMAPE)) \n",
    "    return np.mean(MASE), np.mean(sMAPE)\n",
    "CV_Evaluation(remov_nan(Dataset_weekly.iloc[:,5]),13,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_prediction_CV (Data: pd.DataFrame,fh,freq):\n",
    "    \n",
    "    \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset\n",
    "    \n",
    "    Lin_Reg_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    Lin_Reg_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    \n",
    "   \n",
    "    c =0\n",
    "    # Iteration through each serie in the dataset\n",
    "    for i in range(n):\n",
    "        zr = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        new_Data = remov_nan (zr)\n",
    "        \n",
    "        in_back = int(0.065*len(new_Data))\n",
    "        # load the value of new_Data\n",
    "        \n",
    "        \n",
    "        Mase,sMape = CV_Evaluation(new_Data,fh,in_back,freq)\n",
    "        # calculation of Error\n",
    "        Lin_Reg_sMAPE.append(sMape)\n",
    "\n",
    "        Lin_Reg_MASE.append(Mase)\n",
    "        \n",
    "        #print(c)\n",
    "        c+=1\n",
    "        \n",
    "    # Printing\n",
    "    print(\"number of Time Series:\",c)\n",
    "    \n",
    "    print(\"\\n---------FINAL RESULTS---------\")\n",
    "    \n",
    "    print(\"===========  sMAPE  =============\\n\")\n",
    "    \n",
    "    print(\"#### Dec_Tree ####\\n\", np.mean(Lin_Reg_sMAPE), \"\\n\")\n",
    "\n",
    "    print(\"============  MASE  =============\")\n",
    "    \n",
    "    print(\"#### Dec_Tree ####\\n\", np.mean(Lin_Reg_MASE), \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  daily Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  monthly Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  quaterly Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  yearly Data\n",
    "main_prediction_CV(Dataset_weekly,13,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_prediction_CV_ (Data: pd.DataFrame,fh,freq):\n",
    "    \n",
    "    \n",
    "    n = Data.shape[1]   # number of Serie in the Dataset\n",
    "    \n",
    "    Lin_Reg_MASE =[]     # a list to save all mase_values of linear regression of each Serie forcasting \n",
    "    Lin_Reg_sMAPE =[]    # a list to save all smape_values of linear regression of each Serie forcasting\n",
    "    \n",
    "   \n",
    "    c =0\n",
    "    # Iteration through each serie in the dataset\n",
    "    for i in range(n):\n",
    "        zr = Data.iloc[:,i]\n",
    "        \n",
    "        # remove all NaN value from the serie\n",
    "        new_Data = remov_nan (zr)\n",
    "        \n",
    "        in_back = int(0.065*len(new_Data))\n",
    "        # load the value of new_Data\n",
    "        \n",
    "        \n",
    "        Mase,sMape = CV_Evaluation(new_Data,fh,in_back,freq)\n",
    "        # calculation of Error\n",
    "        Lin_Reg_sMAPE.append(sMape)\n",
    "\n",
    "        Lin_Reg_MASE.append(Mase)\n",
    "        \n",
    "        #print(c)\n",
    "        c+=1\n",
    "        \n",
    "    # Printing\n",
    "    print(\"number of Time Series:\",c)\n",
    "    \n",
    "    print(\"\\n---------FINAL RESULTS---------\")\n",
    "    \n",
    "    print(\"===========  sMAPE  =============\\n\")\n",
    "    \n",
    "    print(\"#### Lin_Reg ####\\n\", np.mean(Lin_Reg_sMAPE), \"\\n\")\n",
    "\n",
    "    print(\"============  MASE  =============\")\n",
    "    \n",
    "    print(\"#### Lin_Reg ####\\n\", np.mean(Lin_Reg_MASE), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

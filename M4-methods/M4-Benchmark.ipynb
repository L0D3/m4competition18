{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:256: DeprecationWarning: This function is deprecated. Please call randint(0, 100 + 1) instead\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:69: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:70: FutureWarning: pd.rolling_mean is deprecated for ndarrays and will be removed in a future version\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TS ID:  1 -------------\n",
      "-------------TS ID:  2 -------------\n",
      "-------------TS ID:  3 -------------\n",
      "-------------TS ID:  4 -------------\n",
      "-------------TS ID:  5 -------------\n",
      "-------------TS ID:  6 -------------\n",
      "-------------TS ID:  7 -------------\n",
      "-------------TS ID:  8 -------------\n",
      "-------------TS ID:  9 -------------\n",
      "-------------TS ID:  10 -------------\n",
      "-------------TS ID:  11 -------------\n",
      "-------------TS ID:  12 -------------\n",
      "-------------TS ID:  13 -------------\n",
      "-------------TS ID:  14 -------------\n",
      "-------------TS ID:  15 -------------\n",
      "-------------TS ID:  16 -------------\n",
      "-------------TS ID:  17 -------------\n",
      "-------------TS ID:  18 -------------\n",
      "-------------TS ID:  19 -------------\n",
      "-------------TS ID:  20 -------------\n",
      "-------------TS ID:  21 -------------\n",
      "-------------TS ID:  22 -------------\n",
      "-------------TS ID:  23 -------------\n",
      "-------------TS ID:  24 -------------\n",
      "-------------TS ID:  25 -------------\n",
      "-------------TS ID:  26 -------------\n",
      "-------------TS ID:  27 -------------\n",
      "-------------TS ID:  28 -------------\n",
      "-------------TS ID:  29 -------------\n",
      "-------------TS ID:  30 -------------\n",
      "-------------TS ID:  31 -------------\n",
      "-------------TS ID:  32 -------------\n",
      "-------------TS ID:  33 -------------\n",
      "-------------TS ID:  34 -------------\n",
      "-------------TS ID:  35 -------------\n",
      "-------------TS ID:  36 -------------\n",
      "-------------TS ID:  37 -------------\n",
      "-------------TS ID:  38 -------------\n",
      "-------------TS ID:  39 -------------\n",
      "-------------TS ID:  40 -------------\n",
      "-------------TS ID:  41 -------------\n",
      "-------------TS ID:  42 -------------\n",
      "-------------TS ID:  43 -------------\n",
      "-------------TS ID:  44 -------------\n",
      "-------------TS ID:  45 -------------\n",
      "-------------TS ID:  46 -------------\n",
      "-------------TS ID:  47 -------------\n",
      "-------------TS ID:  48 -------------\n",
      "-------------TS ID:  49 -------------\n",
      "-------------TS ID:  50 -------------\n",
      "-------------TS ID:  51 -------------\n",
      "-------------TS ID:  52 -------------\n",
      "-------------TS ID:  53 -------------\n",
      "-------------TS ID:  54 -------------\n",
      "-------------TS ID:  55 -------------\n",
      "-------------TS ID:  56 -------------\n",
      "-------------TS ID:  57 -------------\n",
      "-------------TS ID:  58 -------------\n",
      "-------------TS ID:  59 -------------\n",
      "-------------TS ID:  60 -------------\n",
      "-------------TS ID:  61 -------------\n",
      "-------------TS ID:  62 -------------\n",
      "-------------TS ID:  63 -------------\n",
      "-------------TS ID:  64 -------------\n",
      "-------------TS ID:  65 -------------\n",
      "-------------TS ID:  66 -------------\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.optimizers import rmsprop\n",
    "from keras import backend as ker\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "def detrend(insample_data):\n",
    "    \"\"\"\n",
    "    Calculates a & b parameters of LRL\n",
    "\n",
    "    :param insample_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = np.arange(len(insample_data))\n",
    "    a, b = np.polyfit(x, insample_data, 1)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def deseasonalize(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Calculates and returns seasonal indices\n",
    "\n",
    "    :param original_ts: original data\n",
    "    :param ppy: periods per year\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # === get in-sample data\n",
    "    original_ts = original_ts[:-out_of_sample]\n",
    "    \"\"\"\n",
    "    if seasonality_test(original_ts, ppy):\n",
    "        # print(\"seasonal\")\n",
    "        # ==== get moving averages\n",
    "        ma_ts = moving_averages(original_ts, ppy)\n",
    "\n",
    "        # ==== get seasonality indices\n",
    "        le_ts = original_ts * 100 / ma_ts\n",
    "        le_ts = np.hstack((le_ts, np.full((ppy - (len(le_ts) % ppy)), np.nan)))\n",
    "        le_ts = np.reshape(le_ts, (-1, ppy))\n",
    "        si = np.nanmean(le_ts, 0)\n",
    "        norm = np.sum(si) / (ppy * 100)\n",
    "        si = si / norm\n",
    "    else:\n",
    "        # print(\"NOT seasonal\")\n",
    "        si = np.full(ppy, 100)\n",
    "\n",
    "    return si\n",
    "\n",
    "\n",
    "def moving_averages(ts_init, window):\n",
    "    \"\"\"\n",
    "    Calculates the moving averages for a given TS\n",
    "\n",
    "    :param ts_init: the original time series\n",
    "    :param window: window length\n",
    "    :return: moving averages ts\n",
    "    \"\"\"\n",
    "    if len(ts_init) % 2 == 0:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "        ts_ma = pd.rolling_mean(ts_ma, 2, center=True)\n",
    "        ts_ma = np.roll(ts_ma, -1)\n",
    "    else:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "\n",
    "    return ts_ma\n",
    "\n",
    "\n",
    "def seasonality_test(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Seasonality test\n",
    "\n",
    "    :param original_ts: time series\n",
    "    :param ppy: periods per year\n",
    "    :return: boolean value: whether the TS is seasonal\n",
    "    \"\"\"\n",
    "    s = acf(original_ts, 1)\n",
    "    for i in range(2, ppy):\n",
    "        s = s + (acf(original_ts, i) ** 2)\n",
    "\n",
    "    limit = 1.645 * (sqrt((1 + 2 * s) / len(original_ts)))\n",
    "\n",
    "    return (abs(acf(original_ts, ppy))) > limit\n",
    "\n",
    "\n",
    "def acf(data, k):\n",
    "    \"\"\"\n",
    "    Autocorrelation function\n",
    "\n",
    "    :param data: time series\n",
    "    :param k: lag\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = np.mean(data)\n",
    "    s1 = 0\n",
    "    for i in range(k, len(data)):\n",
    "        s1 = s1 + ((data[i] - m) * (data[i - k] - m))\n",
    "\n",
    "    s2 = 0\n",
    "    for i in range(0, len(data)):\n",
    "        s2 = s2 + ((data[i] - m) ** 2)\n",
    "\n",
    "    return float(s1 / s2)\n",
    "\n",
    "\n",
    "def split_into_train_test(data, in_num, fh):\n",
    "    \"\"\"\n",
    "    Splits the series into train and test sets. Each step takes multiple points as inputs\n",
    "\n",
    "    :param data: an individual TS\n",
    "    :param fh: number of out of sample points\n",
    "    :param in_num: number of input points for the forecast\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train, test = data[:-fh], data[-(fh + in_num):]\n",
    "    x_train, y_train = train[:-1], np.roll(train, -in_num)[:-in_num]\n",
    "    x_test, y_test = train[-in_num:], np.roll(test, -in_num)[:-in_num]\n",
    "\n",
    "    # reshape input to be [samples, time steps, features] (N-NF samples, 1 time step, 1 feature)\n",
    "    x_train = np.reshape(x_train, (-1, 1))\n",
    "    x_test = np.reshape(x_test, (-1, 1))\n",
    "    temp_test = np.roll(x_test, -1)\n",
    "    temp_train = np.roll(x_train, -1)\n",
    "    for x in range(1, in_num):\n",
    "        x_train = np.concatenate((x_train[:-1], temp_train[:-1]), 1)\n",
    "        x_test = np.concatenate((x_test[:-1], temp_test[:-1]), 1)\n",
    "        temp_test = np.roll(temp_test, -1)[:-1]\n",
    "        temp_train = np.roll(temp_train, -1)[:-1]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def rnn_bench(x_train, y_train, x_test, fh, input_size):\n",
    "    \"\"\"\n",
    "    Forecasts using 6 SimpleRNN nodes in the hidden layer and a Dense output layer\n",
    "\n",
    "    :param x_train: train data\n",
    "    :param y_train: target values for training\n",
    "    :param x_test: test data\n",
    "    :param fh: forecasting horizon\n",
    "    :param input_size: number of points used as input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reshape to match expected input\n",
    "    x_train = np.reshape(x_train, (-1, input_size, 1))\n",
    "    x_test = np.reshape(x_test, (-1, input_size, 1))\n",
    "\n",
    "    # create the model\n",
    "    model = Sequential([\n",
    "        SimpleRNN(6, input_shape=(input_size, 1), activation='linear',\n",
    "                  use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                  recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  dropout=0.0, recurrent_dropout=0.0),\n",
    "        Dense(1, use_bias=True, activation='linear')\n",
    "    ])\n",
    "    opt = rmsprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "    # make predictions\n",
    "    y_hat_test = []\n",
    "    last_prediction = model.predict(x_test)[0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0, (len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0]\n",
    "\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "def mlp_bench(x_train, y_train, x_test, fh):\n",
    "    \"\"\"\n",
    "    Forecasts using a simple MLP which 6 nodes in the hidden layer\n",
    "\n",
    "    :param x_train: train input data\n",
    "    :param y_train: target values for training\n",
    "    :param x_test: test data\n",
    "    :param fh: forecasting horizon\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_test = []\n",
    "\n",
    "    model = MLPRegressor(hidden_layer_sizes=6, activation='identity', solver='adam',\n",
    "                         max_iter=100, learning_rate='adaptive', learning_rate_init=0.001,\n",
    "                         random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    last_prediction = model.predict(x_test)[0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0, (len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0]\n",
    "\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item()\n",
    "\n",
    "\n",
    "def mase(insample, y_test, y_hat_test, freq):\n",
    "    \"\"\"\n",
    "    Calculates MAsE\n",
    "\n",
    "    :param insample: insample data\n",
    "    :param y_test: out of sample target values\n",
    "    :param y_hat_test: predicted values\n",
    "    :param freq: data frequency\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_naive = []\n",
    "    for i in range(freq, len(insample)):\n",
    "        y_hat_naive.append(insample[(i - freq)])\n",
    "\n",
    "    masep = np.mean(abs(insample[freq:] - y_hat_naive))\n",
    "\n",
    "    return np.mean(abs(y_test - y_hat_test)) / masep\n",
    "\n",
    "\n",
    "def main():\n",
    "    fh = 6         # forecasting horizon\n",
    "    freq = 1       # data frequency\n",
    "    in_size = 3    # number of points used as input for each forecast\n",
    "\n",
    "    err_MLP_sMAPE = []\n",
    "    err_MLP_MASE = []\n",
    "    err_RNN_sMAPE = []\n",
    "    err_RNN_MASE = []\n",
    "\n",
    "    # ===== In this example we produce forecasts for 100 randomly generated timeseries =====\n",
    "    \n",
    "    df_yearly = pd.read_csv(\"../data/Yearly-train.csv\", skiprows=0, index_col =0)\n",
    "    #data_all = df_yearly.T\n",
    "    data_all = np.array(np.random.random_integers(0, 100, (100, 20)), dtype=np.float32)\n",
    "    for i in range(0, 100):\n",
    "        for j in range(0, 20):\n",
    "            data_all[i, j] = j * 10 + data_all[i, j]\n",
    "\n",
    "    counter = 0\n",
    "    # ===== Main loop which goes through all timeseries =====\n",
    "    for j in range(len(data_all)):\n",
    "        ts = data_all[j, :]\n",
    "\n",
    "        # remove seasonality\n",
    "        seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "        # detrending\n",
    "        a, b = detrend(ts)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] - ((a * i) + b)\n",
    "\n",
    "        x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "        # RNN benchmark - Produce forecasts\n",
    "        y_hat_test_RNN = np.reshape(rnn_bench(x_train, y_train, x_test, fh, in_size), (-1))\n",
    "\n",
    "        # MLP benchmark - Produce forecasts\n",
    "        y_hat_test_MLP = mlp_bench(x_train, y_train, x_test, fh)\n",
    "        for i in range(0, 29):\n",
    "            y_hat_test_MLP = np.vstack((y_hat_test_MLP, mlp_bench(x_train, y_train, x_test, fh)))\n",
    "        y_hat_test_MLP = np.median(y_hat_test_MLP, axis=0)\n",
    "\n",
    "        # add trend\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "        for i in range(0, fh):\n",
    "            y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "            y_hat_test_RNN[i] = y_hat_test_RNN[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "        # add seasonality\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        for i in range(len(ts), len(ts) + fh):\n",
    "            y_hat_test_MLP[i - len(ts)] = y_hat_test_MLP[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "            y_hat_test_RNN[i - len(ts)] = y_hat_test_RNN[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        # check if negative or extreme\n",
    "        for i in range(len(y_hat_test_MLP)):\n",
    "            if y_hat_test_MLP[i] < 0:\n",
    "                y_hat_test_MLP[i] = 0\n",
    "            if y_hat_test_RNN[i] < 0:\n",
    "                y_hat_test_RNN[i] = 0\n",
    "                \n",
    "            if y_hat_test_MLP[i] > (1000 * max(ts)):\n",
    "                y_hat_test_MLP[i] = max(ts)         \n",
    "            if y_hat_test_RNN[i] > (1000 * max(ts)):\n",
    "                y_hat_test_RNN[i] = max(ts)\n",
    "\n",
    "        x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "        # Calculate errors\n",
    "        err_MLP_sMAPE.append(smape(y_test, y_hat_test_MLP))\n",
    "        err_RNN_sMAPE.append(smape(y_test, y_hat_test_RNN))\n",
    "        err_MLP_MASE.append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))\n",
    "        err_RNN_MASE.append(mase(ts[:-fh], y_test, y_hat_test_RNN, freq))\n",
    "\n",
    "        # memory handling\n",
    "        ker.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        gc.collect()\n",
    "\n",
    "        counter = counter + 1\n",
    "        print(\"-------------TS ID: \", counter, \"-------------\")\n",
    "\n",
    "    print(\"\\n\\n---------FINAL RESULTS---------\")\n",
    "    print(\"=============sMAPE=============\\n\")\n",
    "    print(\"#### MLP ####\\n\", np.mean(err_MLP_sMAPE), \"\\n\")\n",
    "    print(\"#### RNN ####\\n\", np.mean(err_RNN_sMAPE), \"\\n\")\n",
    "    print(\"==============MASE=============\")\n",
    "    print(\"#### MLP ####\\n\", np.mean(err_MLP_MASE), \"\\n\")\n",
    "    print(\"#### RNN ####\\n\", np.mean(err_RNN_MASE), \"\\n\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.array(np.random.random_integers(0, 100, (100, 20)), dtype=np.float32)\n",
    "print(\"richtig\",data_all.shape)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yearly = pd.read_csv(\"../data/Yearly-train.csv\", skiprows=0, index_col =0)\n",
    "data_all = df_yearly.T\n",
    "print(\"falsch\",data_all.shape)\n",
    "data_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

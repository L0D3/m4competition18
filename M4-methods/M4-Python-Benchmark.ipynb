{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.optimizers import rmsprop\n",
    "from keras import backend as ker\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_SEED= 40\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.optimizers import rmsprop\n",
    "from keras import backend as ker\n",
    "\n",
    "def remov_nan (dataset):\n",
    "    '''\n",
    "    to remove all NaN Values in a \n",
    "    Time Serie Dataframe\n",
    "    '''\n",
    "    n = dataset.isnull().sum() \n",
    "    data = dataset[0:(len(dataset)-n)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def detrend(insample_data):\n",
    "    \"\"\"\n",
    "    Calculates a & b parameters of LRL\n",
    "\n",
    "    :param insample_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = np.arange(len(insample_data))\n",
    "    a, b = np.polyfit(x, insample_data, 1)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def deseasonalize(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Calculates and returns seasonal indices\n",
    "\n",
    "    :param original_ts: original data\n",
    "    :param ppy: periods per year\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # === get in-sample data\n",
    "    original_ts = original_ts[:-out_of_sample]\n",
    "    \"\"\"\n",
    "    if seasonality_test(original_ts, ppy):\n",
    "        # print(\"seasonal\")\n",
    "        # ==== get moving averages\n",
    "        ma_ts = moving_averages(original_ts, ppy)\n",
    "\n",
    "        # ==== get seasonality indices\n",
    "        le_ts = original_ts * 100 / ma_ts\n",
    "        le_ts = np.hstack((le_ts, np.full((ppy - (len(le_ts) % ppy)), np.nan)))\n",
    "        le_ts = np.reshape(le_ts, (-1, ppy))\n",
    "        si = np.nanmean(le_ts, 0)\n",
    "        norm = np.sum(si) / (ppy * 100)\n",
    "        si = si / norm\n",
    "    else:\n",
    "        # print(\"NOT seasonal\")\n",
    "        si = np.full(ppy, 100)\n",
    "\n",
    "    return si\n",
    "\n",
    "\n",
    "def moving_averages(ts_init, window):\n",
    "    \"\"\"\n",
    "    Calculates the moving averages for a given TS\n",
    "\n",
    "    :param ts_init: the original time series\n",
    "    :param window: window length\n",
    "    :return: moving averages ts\n",
    "    \"\"\"\n",
    "    if len(ts_init) % 2 == 0:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "        ts_ma = pd.rolling_mean(ts_ma, 2, center=True)\n",
    "        ts_ma = np.roll(ts_ma, -1)\n",
    "    else:\n",
    "        ts_ma = pd.rolling_mean(ts_init, window, center=True)\n",
    "\n",
    "    return ts_ma\n",
    "\n",
    "\n",
    "def seasonality_test(original_ts, ppy):\n",
    "    \"\"\"\n",
    "    Seasonality test\n",
    "\n",
    "    :param original_ts: time series\n",
    "    :param ppy: periods per year\n",
    "    :return: boolean value: whether the TS is seasonal\n",
    "    \"\"\"\n",
    "    s = acf(original_ts, 1)\n",
    "    for i in range(2, ppy):\n",
    "        s = s + (acf(original_ts, i) ** 2)\n",
    "\n",
    "    limit = 1.645 * (sqrt((1 + 2 * s) / len(original_ts)))\n",
    "\n",
    "    return (abs(acf(original_ts, ppy))) > limit\n",
    "\n",
    "\n",
    "def acf(data, k):\n",
    "    \"\"\"\n",
    "    Autocorrelation function\n",
    "\n",
    "    :param data: time series\n",
    "    :param k: lag\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = np.mean(data)\n",
    "    s1 = 0\n",
    "    for i in range(k, len(data)):\n",
    "        s1 = s1 + ((data[i] - m) * (data[i - k] - m))\n",
    "\n",
    "    s2 = 0\n",
    "    for i in range(0, len(data)):\n",
    "        s2 = s2 + ((data[i] - m) ** 2)\n",
    "\n",
    "    return float(s1 / s2)\n",
    "\n",
    "def windows_for_forecasts(data, in_num, fh):\n",
    "    \"\"\"\n",
    "    Splits the series into train and test sets. Each step takes multiple points as inputs\n",
    "    \"\"\"\n",
    "    x_train, y_train = data[:-1], np.roll(data, -in_num)[:-in_num]\n",
    "\n",
    "\n",
    "    # reshape input to be [samples, time steps, features] (N-NF samples, 1 time step, 1 feature)\n",
    "    x_train = np.reshape(x_train, (-1, 1))\n",
    "  \n",
    "    temp_train = np.roll(x_train, -1)\n",
    "    for x in range(1, in_num):\n",
    "        x_train = np.concatenate((x_train[:-1], temp_train[:-1]), 1)\n",
    "        temp_train = np.roll(temp_train, -1)[:-1]\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "def split_into_train_test(data, in_num, fh):\n",
    "    \"\"\"\n",
    "    Splits the series into train and test sets. Each step takes multiple points as inputs\n",
    "\n",
    "    :param data: an individual TS\n",
    "    :param fh: number of out of sample points\n",
    "    :param in_num: number of input points for the forecast\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train, test = data[:-fh], data[-(fh + in_num):]\n",
    "    x_train, y_train = train[:-1], np.roll(train, -in_num)[:-in_num]\n",
    "    x_test, y_test = train[-in_num:], np.roll(test, -in_num)[:-in_num]\n",
    "\n",
    "    # reshape input to be [samples, time steps, features] (N-NF samples, 1 time step, 1 feature)\n",
    "    x_train = np.reshape(x_train, (-1, 1))\n",
    "    x_test = np.reshape(x_test, (-1, 1))\n",
    "    temp_test = np.roll(x_test, -1)\n",
    "    temp_train = np.roll(x_train, -1)\n",
    "    for x in range(1, in_num):\n",
    "        x_train = np.concatenate((x_train[:-1], temp_train[:-1]), 1)\n",
    "        x_test = np.concatenate((x_test[:-1], temp_test[:-1]), 1)\n",
    "        temp_test = np.roll(temp_test, -1)[:-1]\n",
    "        temp_train = np.roll(temp_train, -1)[:-1]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def rnn_bench(x_train, y_train, x_test, fh, input_size):\n",
    "    \"\"\"\n",
    "    Forecasts using 6 SimpleRNN nodes in the hidden layer and a Dense output layer\n",
    "\n",
    "    :param x_train: train data\n",
    "    :param y_train: target values for training\n",
    "    :param x_test: test data\n",
    "    :param fh: forecasting horizon\n",
    "    :param input_size: number of points used as input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reshape to match expected input\n",
    "    x_train = np.reshape(x_train, (-1, input_size, 1))\n",
    "    x_test = np.reshape(x_test, (-1, input_size, 1))\n",
    "\n",
    "    # create the model\n",
    "    model = Sequential([\n",
    "        SimpleRNN(6, input_shape=(input_size, 1), activation='linear',\n",
    "                  use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                  recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  dropout=0.0, recurrent_dropout=0.0),\n",
    "        Dense(1, use_bias=True, activation='linear')\n",
    "    ])\n",
    "    opt = rmsprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(x_train, y_train, epochs=1000, batch_size=100)\n",
    "   # model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "    # make predictions\n",
    "    y_hat_test = []\n",
    "    last_prediction = model.predict(x_test)[0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0, (len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0]\n",
    "\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "def mlp_bench(x_train, y_train, x_test, fh):\n",
    "    \"\"\"\n",
    "    Forecasts using a simple MLP which 6 nodes in the hidden layer\n",
    "\n",
    "    :param x_train: train input data\n",
    "    :param y_train: target values for training\n",
    "    :param x_test: test data\n",
    "    :param fh: forecasting horizon\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_test = []\n",
    "\n",
    "    model = MLPRegressor(hidden_layer_sizes=6, activation='identity', solver='adam',\n",
    "                         max_iter=100, learning_rate='adaptive', learning_rate_init=0.001,\n",
    "                         random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    last_prediction = model.predict(x_test)[0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0, (len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0]\n",
    "\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item()\n",
    "\n",
    "\n",
    "def mase(insample, y_test, y_hat_test, freq):\n",
    "    \"\"\"\n",
    "    Calculates MAsE\n",
    "\n",
    "    :param insample: insample data\n",
    "    :param y_test: out of sample target values\n",
    "    :param y_hat_test: predicted values\n",
    "    :param freq: data frequency\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat_naive = []\n",
    "    for i in range(freq, len(insample)):\n",
    "        y_hat_naive.append(insample[(i - freq)])\n",
    "\n",
    "    masep = np.mean(abs(insample[freq:] - y_hat_naive))\n",
    "\n",
    "    return np.mean(abs(y_test - y_hat_test)) / masep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def main(data_path,fh,freq,df,series_type,forecasts_path):\n",
    "    print('Prepare Data')\n",
    "    data_all=pd.read_csv(data_path, skiprows=0, index_col =0)\n",
    "    in_size = 5    # number of points used as input for each forecast\n",
    "\n",
    "    err_RNN_sMAPE = []\n",
    "    err_RNN_MASE = []\n",
    "    \n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "    # ===== Main loop which goes through all timeseries =====\n",
    "    for j in range(len(data_all)):\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "        ts = data_all.iloc[j, :]\n",
    "        ts = remov_nan(ts)\n",
    "\n",
    "        # remove seasonality\n",
    "        seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "        # detrending\n",
    "        a, b = detrend(ts)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] - ((a * i) + b)\n",
    "\n",
    "        x_train, y_train = windows_for_forecasts(ts, in_size, fh)\n",
    "        \n",
    "        \n",
    "       \n",
    "        # RNN benchmark - Produce forecasts\n",
    "        print('Train LSTM')\n",
    "        #y_hat_test_RNN = np.reshape(rnn_bench(x_train, y_train, x_test, fh, in_size), (-1))\n",
    "        windowForForecast=ts[len(ts)-in_size:]\n",
    "        print('windowForForecast',windowForForecast.shape)\n",
    "        y_hat_test_RNN=LSTM_NN(x_train, y_train, windowForForecast, fh, in_size)\n",
    "        y_hat_test_RNN = np.reshape(y_hat_test_RNN, (-1))\n",
    "\n",
    "        print('series_type',series_type)\n",
    "        df.iloc[j,0]=str(series_type[0])+str(j)\n",
    "        print('y_hat_test_RNN',y_hat_test_RNN)\n",
    "        print('y_hat_test_RNN shape',y_hat_test_RNN.shape)\n",
    "        print('df shape',df.iloc[j].shape)\n",
    "        df.iloc[j,1:(len(y_hat_test_RNN)+1)]=y_hat_test_RNN\n",
    "        df.to_csv(forecasts_path, index=False)\n",
    "        print('Test Models')\n",
    "     \n",
    "\n",
    "        # add trend\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "        for i in range(0, fh):\n",
    "            #y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "            y_hat_test_RNN[i] = y_hat_test_RNN[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "        # add seasonality\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        for i in range(len(ts), len(ts) + fh):\n",
    "            y_hat_test_RNN[i - len(ts)] = y_hat_test_RNN[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        # check if negative or extreme\n",
    "        for i in range(len(y_hat_test_RNN)):\n",
    "       \n",
    "            if y_hat_test_RNN[i] < 0:\n",
    "                y_hat_test_RNN[i] = 0\n",
    "                \n",
    "            \n",
    "            if y_hat_test_RNN[i] > (1000 * max(ts)):\n",
    "                y_hat_test_RNN[i] = max(ts)\n",
    "\n",
    "    return y_hat_test_RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def LSTM_NN(x_train, y_train, x_test, fh, in_back):\n",
    "\n",
    "    \n",
    "    \n",
    "    # reshape to match expected input\n",
    "    x_train = np.reshape(x_train, (-1, in_back, 1))\n",
    "    x_test = np.reshape(x_test, (-1, in_back, 1))\n",
    "      \n",
    "     # create the model and parametrize it\n",
    "    model = Sequential()\n",
    "    \n",
    "    #model.add(LSTM(input_shape=(in_back,1),output_dim=100,return_sequences=True))\n",
    "    #model.add(LSTM(input_shape=(in_back,1),output_dim=50,return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(input_shape=(in_back,1),units=512))\n",
    "    #model.add(LSTM(512))\n",
    "\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # fit the model to the training data\n",
    "    early_stopping_monitor = EarlyStopping(monitor='loss', mode='min',patience=4)\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=1500, batch_size=200,callbacks=[early_stopping_monitor],verbose=0)\n",
    "    #model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "    \n",
    "\n",
    "    # make predictions\n",
    "    y_hat_test = []\n",
    "    last_prediction = model.predict(x_test)[0]\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test.append(last_prediction)\n",
    "        x_test[0] = np.roll(x_test[0], -1)\n",
    "        x_test[0, (len(x_test[0]) - 1)] = last_prediction\n",
    "        last_prediction = model.predict(x_test)[0]\n",
    "    \n",
    "    forecast = []\n",
    "    \n",
    "\n",
    "\n",
    "    return np.asarray(y_hat_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily\n",
      "Prepare Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:92: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=1,center=True).mean()\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [939.0928  937.1328  935.7594  934.28033 932.8767  931.50525 930.22485\n",
      " 929.039   927.9828  927.04895 926.22815 925.54755 924.9829  924.5196 ]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [411.51938 385.9243  432.64078 491.6146  513.24005 472.46765 454.9778\n",
      " 445.74927 433.24213 424.82272 427.32288 454.21292 480.77533 481.98785]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:88: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=1,center=True).mean()\n",
      "/home/jupyter/anaconda3/envs/jupyter/lib/python3.6/site-packages/ipykernel_launcher.py:89: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=2,center=True).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [714.983   689.9539  684.31726 681.1625  678.4194  677.0754  676.32654\n",
      " 675.83154 675.5052  675.29675 675.16437 675.08057 675.0278  674.99445]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [491.48785 485.8333  483.0422  481.74847 481.0738  480.69873 480.48038\n",
      " 480.35342 480.27835 480.2326  480.2048  480.18774 480.1774  480.171  ]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [-291.63025 -294.69022 -296.31787 -297.20218 -297.7425  -298.15338\n",
      " -298.5008  -298.8117  -299.093   -299.34772 -299.58145 -299.80005\n",
      " -300.00464 -300.19653]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [-111.07876  -105.14586   -99.94797   -94.17185   -87.62311   -80.885155\n",
      "  -75.047325  -69.19708   -62.818283  -55.698326  -46.85789   -32.37626\n",
      "   12.485272   93.65365 ]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n",
      "series_type Daily\n",
      "y_hat_test_RNN [2746.922  2711.2903 2674.9087 2626.7637 2617.3853 2613.366  2607.5828\n",
      " 2604.0237 2601.7686 2599.1016 2597.0015 2595.396  2593.7288 2592.328 ]\n",
      "y_hat_test_RNN shape (14,)\n",
      "df shape (49,)\n",
      "Test Models\n",
      "Train LSTM\n",
      "windowForForecast (5,)\n"
     ]
    }
   ],
   "source": [
    "p=['Daily','Weekly','Hourly','Monthly','Quaterly','Yearly']\n",
    "fcs=[14,13,48,18,8,1]\n",
    "frqs=[1,1,24,12,4,1]\n",
    "size=20\n",
    "all_forecasts=np.zeros((100000,49))\n",
    "all_forecasts.fill(np.nan)\n",
    "all_forecasts\n",
    "columns=['id']\n",
    "for i in range(1,49):\n",
    "    columns.append('F'+str(i))\n",
    "columns\n",
    "df=pd.DataFrame(all_forecasts,columns=columns)\n",
    "df.fillna\n",
    "forecasts_path='../results/forecasts-'+str(size)+'.csv'\n",
    "#df.to_csv(forecasts_path, index=False)\n",
    "\n",
    "test1=[]\n",
    "test2=[]\n",
    "for x in range(0,len(p)):\n",
    "    print(p[x])\n",
    "    path=\"\".join([\"../data/csv_\",str(size),\"_sample/\", str(p[x]),'-train-',str(size),'-sample.csv'])\n",
    "    \n",
    "    main(path,fcs[x],frqs[x],df,p[x],forecasts_path)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('smape',test1,'mase',test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-fb2dab076bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;36m916.1903\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m  [916.1859 ]]\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "df=[[927.9543 ],\n",
    " [919.9753 ],\n",
    " [916.0806 ],\n",
    " [914.976  ],\n",
    " [915.10547],\n",
    " [915.569  ],\n",
    " [915.94727],\n",
    " [916.1538 ],\n",
    " [916.2281 ],\n",
    " [916.23413],\n",
    " [916.2175 ],\n",
    " [916.20074],\n",
    " [916.1903 ],\n",
    " [916.1859 ]]\n",
    "df.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  0  1  2\n",
       "1  0  0  0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([[0,0,0],[0,0,0]],columns=['A','B','C'])\n",
    "a=[1,2]\n",
    "df.iloc[0,1]='dadad'[0]+str(1)\n",
    "df.iloc[0,1:len(a)+1]=a\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v1 = array(1, dim=c(8,length(p)*size))\n",
    "v2 = array(1, dim=c(8,length(p)*size))\n",
    "    glimpse(v1)\n",
    "\n",
    "for (x in 1:length(p)){   \n",
    "    v1[,((x-1)*size+1):(x*size)]=test1[[x]]\n",
    "    v2[,((x-1)*size+1):(x*size)]=test2[[x]]\n",
    "}\n",
    "glimpse(v1)\n",
    "glimpse(v2)\n",
    "\n",
    "Total_mase=v2\n",
    "Total_smape=v1\n",
    "result=array(NA,dim = c(length(Names_benchmarks), 4))\n",
    "print(\"########### sMAPE ###############\")\n",
    "for (i in range(1,len(Names_benchmarks)){\n",
    "  smape=round(np.mean(Total_smape[i,]), 3)\n",
    "  mase=round(np.mean(Total_mase[i,]), 3)\n",
    "  owa=round(((np.mean(Total_mase[i,])/mean(Total_mase[3,]))+(mean(Total_smape[i,])/mean(Total_smape[3,])))/2, 3)\n",
    "  \n",
    "  result[i,1]=Names_benchmarks[i]\n",
    "  result[i,2]=smape\n",
    "  result[i,3]=mase\n",
    "  result[i,4]=owa  \n",
    "  \n",
    "  print(paste(Names_benchmarks[i], smape, mase, owa))\n",
    "}\n",
    "write.csv(result, file = paste(\"../results/R-Benchmark-\",size,'-sample.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_all():\n",
    "    \n",
    "    print(\"### Load of Dataset  ###\")\n",
    "    #df_yearly = pd.read_csv(\"../M4-methods/csv_20_sample/Yearly-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    "    #df_quaterly = pd.read_csv(\"../M4-methods/csv_20_sample/Quaterly-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    "    df_monthly = pd.read_csv(\"../M4-methods/csv_20_sample/Monthly-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    "    df_weekly = pd.read_csv(\"../M4-methods/csv_20_sample/Weekly-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    "    df_daily = pd.read_csv(\"../M4-methods/csv_20_sample/Daily-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    "    df_hourly = pd.read_csv(\"../M4-methods/csv_20_sample/Hourly-train-20-sample.csv\", skiprows=0, index_col =0)\n",
    " \n",
    "    \n",
    "    D=[]\n",
    "   # D.append(df_yearly)\n",
    "   # D.append(df_quaterly)\n",
    "    D.append(df_monthly)\n",
    "    D.append(df_weekly)\n",
    "    D.append(df_daily)\n",
    "    D.append(df_hourly)\n",
    "    \n",
    "    columnsname= [\"Data_Type\",\"sMapeLSTM\",\"Mase LSTM\"]\n",
    "    ds = pd.DataFrame(columns=columnsname )\n",
    "    ds.to_csv('outpoutM4.csv')\n",
    "    print(ds.shape)\n",
    "    \n",
    "    for i in range (len(D)):\n",
    "        \n",
    "        if i==0:\n",
    "            print( \"*** Beginn of Monthly dataset ***\")\n",
    "            a,b = main(df_monthly,18,12,i)\n",
    "            p= [\"Monthly_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)\n",
    "        if i==1:\n",
    "            print( \"*** Beginn of Weekly dataset ***\")\n",
    "            a,b = main(df_weekly,13,1,i)\n",
    "            p= [\"Weekly_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)\n",
    "        if i==2:\n",
    "            print( \"*** Beginn of Daily dataset ***\")\n",
    "            a,b= main(df_daily,14,1,i)\n",
    "            p= [\"Daily_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)\n",
    "        if i==3:\n",
    "            print( \"*** Beginn of Hourly dataset ***\")\n",
    "            a,b = main(df_hourly,48,24,i)\n",
    "            p= [\"Hourly_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_forecasts=np.zeros((100000,49))\n",
    "all_forecasts.fill(np.nan)\n",
    "all_forecasts\n",
    "columns=['id']\n",
    "for i in range(1,49):\n",
    "    columns.append('F'+str(i))\n",
    "columns\n",
    "df=pd.DataFrame(all_forecasts,columns=columns)\n",
    "df.fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i==0:\n",
    "            print( \"*** Beginn of yearly dataset ***\")\n",
    "            a,b = main(df_yearly,6,1,i)\n",
    "            p= [\"Yearly_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)\n",
    "        if i==1:\n",
    "            print( \"*** Beginn of Quarterly dataset ***\")\n",
    "            a,b = main(df_quaterly,8,4,i)\n",
    "            p= [\"Quarterly_data\",a,b]\n",
    "            ds.iloc[:,i]= p\n",
    "            ds=ds.round(4)\n",
    "            ds.to_csv('outputM4LSTM.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def main(data_path,fh,freq,df,series_type,forecasts_path):\n",
    "    print('Prepare Data')\n",
    "    data_all=pd.read_csv(data_path, skiprows=0, index_col =0)\n",
    "    in_size = 5    # number of points used as input for each forecast\n",
    "\n",
    "    err_RNN_sMAPE = []\n",
    "    err_RNN_MASE = []\n",
    "    \n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "    # ===== Main loop which goes through all timeseries =====\n",
    "    for j in range(len(data_all)):\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "        ts = data_all.iloc[j, :]\n",
    "        ts = remov_nan(ts)\n",
    "\n",
    "        # remove seasonality\n",
    "        seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "        # detrending\n",
    "        a, b = detrend(ts)\n",
    "\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] - ((a * i) + b)\n",
    "\n",
    "        x_train, y_train = windows_for_forecasts(ts, in_size, fh)\n",
    "        \n",
    "        \n",
    "       \n",
    "        # RNN benchmark - Produce forecasts\n",
    "        print('Train LSTM')\n",
    "        #y_hat_test_RNN = np.reshape(rnn_bench(x_train, y_train, x_test, fh, in_size), (-1))\n",
    "        windowForForecast=ts[len(ts)-in_size:]\n",
    "        print(windowForForecast,windowForForecast.shape)\n",
    "        lstm=LSTM_NN(x_train, y_train, windowForForecast, fh, in_size)\n",
    "        df.loc[j,0]=series_type[0]+str(j)\n",
    "        df.loc[j,1:len(forecast)+1]=forecast\n",
    "        df.to_csv(forecasts_path,mode='a', index=False)\n",
    "        y_hat_test_RNN = np.reshape(lstm, (-1))\n",
    "        print('Test Models')\n",
    "     \n",
    "\n",
    "        # add trend\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "        for i in range(0, fh):\n",
    "            #y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "            y_hat_test_RNN[i] = y_hat_test_RNN[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "        # add seasonality\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        for i in range(len(ts), len(ts) + fh):\n",
    "            y_hat_test_RNN[i - len(ts)] = y_hat_test_RNN[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "        # check if negative or extreme\n",
    "        for i in range(len(y_hat_test_RNN)):\n",
    "       \n",
    "            if y_hat_test_RNN[i] < 0:\n",
    "                y_hat_test_RNN[i] = 0\n",
    "                \n",
    "            \n",
    "            if y_hat_test_RNN[i] > (1000 * max(ts)):\n",
    "                y_hat_test_RNN[i] = max(ts)\n",
    "\n",
    "        x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "        # Calculate errors\n",
    "       # err_MLP_sMAPE.append(smape(y_test, y_hat_test_MLP))\n",
    "        err_RNN_sMAPE.append(smape(y_test, y_hat_test_RNN))\n",
    "      #  err_MLP_MASE.append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))\n",
    "        err_RNN_MASE.append(mase(ts[:-fh], y_test, y_hat_test_RNN, freq))\n",
    "\n",
    "        # memory handling\n",
    "        ker.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        gc.collect()\n",
    "\n",
    "        counter = counter + 1\n",
    "        #**********************************************************************\n",
    "        print(\"-------------TS ID: \", counter, \"-------------\")\n",
    "        print(\" sMAPE_RNN:\",err_RNN_sMAPE[-1],\" MASE_RNN:\",err_RNN_MASE[-1])\n",
    "        print(\"Time:\",time.time()-start)\n",
    "        #break\n",
    "        \n",
    "        #********************************************************************    \n",
    "    print(\"\\n\\n---------FINAL RESULTS---------\")\n",
    "    print(\"=============sMAPE=============\\n\")\n",
    "    print(\"#### RNN ####\\n\", np.mean(err_RNN_sMAPE), \"\\n\")\n",
    "    print(\"==============MASE=============\")\n",
    "    print(\"#### RNN ####\\n\", np.mean(err_RNN_MASE), \"\\n\")\n",
    "    return np.mean(err_RNN_sMAPE),np.mean(err_RNN_MASE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
